{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b938483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d29713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/praveen/theoden/emnlp_25/forget_20_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a24d1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"idk.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    idk_responses = [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4481f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_responses = random.sample(idk_responses, 98)\n",
    "df['idk'] = random_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326c8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['title', 'question', 'answer', 'idk']]\n",
    "#df.to_csv('dpo_forget_idk.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abfc9d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1620830c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'question', 'answer', 'idk'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raw_data_to_model_qa(tokenizer, max_length,  question, answer, configs):\n",
    "    question = str(question)\n",
    "    answer = str(answer)\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    new_question = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenizer = False,\n",
    "        add_generataion_prompt=True\n",
    "    )\n",
    "    \n",
    "    full_text = str(new_question) + answer\n",
    "    num_question_tokens = len(tokenizer.tokenize(str(new_question), add_special_tokens=True))\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        full_text, \n",
    "        add_special_tokens=True, \n",
    "        max_length=max_length, \n",
    "        truncation=True, \n",
    "    )\n",
    "    pad_length = max_length - len(encoded.input_ids)\n",
    "    \n",
    "    pad_input_ids = encoded['input_ids'] + [tokenizer.eos_token_id] * pad_length\n",
    "    pad_attention_mask = encoded['attention_mask'] + [0] * pad_length\n",
    "    if len(encoded.input_ids) == max_length:\n",
    "        label = encoded.input_ids\n",
    "    else:\n",
    "        label = encoded['input_ids'] + [tokenizer.eos_token_id] + [-100] * (pad_length-1)\n",
    "\n",
    "    #change label to -100 for question tokens\n",
    "    for i in range(num_question_tokens): label[i] = -100\n",
    "\n",
    "    return torch.tensor(pad_input_ids),torch.tensor(label),torch.tensor(pad_attention_mask)\n",
    "\n",
    "\n",
    "class VanillaDPODataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for creating data for forgetting.\n",
    "    Processes 'question'/'answer' pairs and 'question'/'idk' pairs separately.\n",
    "\n",
    "    Args:\n",
    "        forget_data (pd.DataFrame): DataFrame containing 'question', 'answer', and 'idk' columns.\n",
    "        tokenizer: tokenizer instance to process text\n",
    "        max_length (int): maximum sequence length\n",
    "        template_format (str, optional): format template for structuring input\n",
    "        question_key (str): Column name for the question. Defaults to 'question'.\n",
    "        answer_key (str): Column name for the answer to forget. Defaults to 'answer'.\n",
    "        idk_key (str): Column name for the 'I don't know' or alternative response. Defaults to 'idk'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing processed data for both the original answer and the idk response:\n",
    "        {\n",
    "            \"answer_data\": (answer_input_ids, answer_labels, answer_attention_mask),\n",
    "            \"idk_data\": (idk_input_ids, idk_labels, idk_attention_mask)\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self, forget_data: pd.DataFrame, tokenizer: Any, max_length: int, template_format: str = None,\n",
    "                 question_key: str = 'question',\n",
    "                 answer_key: str = 'answer',\n",
    "                 idk_key: str = 'idk'):\n",
    "        if not all(k in forget_data.columns for k in [question_key, answer_key, idk_key]):\n",
    "             raise ValueError(f\"forget_data must contain columns: {question_key}, {answer_key}, {idk_key}\")\n",
    "\n",
    "        self.forget_data = forget_data.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.template_format = template_format\n",
    "        self.qk = question_key\n",
    "        self.ak = answer_key\n",
    "        self.ik = idk_key\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.forget_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.forget_data.iloc[idx]\n",
    "        q = row[self.qk]\n",
    "        ans = row[self.ak]\n",
    "        idk = row[self.ik]\n",
    "\n",
    "        ai, al, am = convert_raw_data_to_model_qa(self.tokenizer,\n",
    "                                                self.max_length,\n",
    "                                                q, ans,\n",
    "                                                self.template_format)\n",
    "        ii, il, im = convert_raw_data_to_model_qa(self.tokenizer,\n",
    "                                                self.max_length,\n",
    "                                                q, idk,\n",
    "                                                self.template_format)\n",
    "\n",
    "        return {\n",
    "            'answer_input_ids':      ai,\n",
    "            'answer_labels':         al,\n",
    "            'answer_attention_mask': am,\n",
    "            'idk_input_ids':         ii,\n",
    "            'idk_labels':            il,\n",
    "            'idk_attention_mask':    im,\n",
    "        }\n",
    "\n",
    "def custom_forget_collator(samples):\n",
    "    # unpack into two lists of tuples\n",
    "    answer_list = [s['answer_data'] for s in samples]\n",
    "    idk_list    = [s['idk_data']    for s in samples]\n",
    "\n",
    "    def stack_triplet(triplets):\n",
    "        # each triplet is (input_ids, labels, attn_mask)\n",
    "        input_ids      = torch.stack([t[0] for t in triplets], dim=0)\n",
    "        labels         = torch.stack([t[1] for t in triplets], dim=0)\n",
    "        attention_mask = torch.stack([t[2] for t in triplets], dim=0)\n",
    "        return input_ids, labels, attention_mask\n",
    "\n",
    "    batched = {\n",
    "      \"forget\": {\n",
    "        \"answer\": stack_triplet(answer_list),\n",
    "        \"idk\":    stack_triplet(idk_list),\n",
    "      }\n",
    "    }\n",
    "    return batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bae7f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VanillaDPODataset(df, tokenizer, max_length=256)\n",
    "loaded_data = DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b495e50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aaa8315d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forget': {'answer': {'input_ids': tensor([128000,     58,   4386,    931,     11,    220,   4386,  11030,     11,\n",
       "              220,  22750,     20,     11,    220,   4386,  11194,     11,    220,\n",
       "            15828,     11,    220,  20062,   2287,     11,    220,   5894,     18,\n",
       "               11,    220,  10568,    914,     11,    220,  16955,     21,     11,\n",
       "              220,    914,     11,    220,  25136,     15,     11,    220,   8610,\n",
       "               11,    220,  14087,     21,     11,    220,    972,     11,    220,\n",
       "             3753,     11,    220,  10895,   1187,     11,    220,  16955,     21,\n",
       "               11,    220,    914,     11,    220,   8610,     11,    220,  10674,\n",
       "               22,     11,    220,   4278,   5495,     11,    220,   8610,     11,\n",
       "              220,  14087,     21,     11,    220,    777,     11,    220,  15828,\n",
       "               11,    220,   4386,  13858,     11,    220,   4386,  11030,     11,\n",
       "              220,  23213,     11,    220,   4386,  11194,     11,    220,  15828,\n",
       "               11,    220,  19695,     18,     11,    220,  22608,   3391,     11,\n",
       "              220,  23402,     11,    220,  22369,    508,     11,    220,  23033,\n",
       "             4643,     11,    220,  13655,     11,    220,   8190,     17,     11,\n",
       "              220,   5332,     11,    220,    966,     11,    220,   4386,  13858,\n",
       "               60,  70211, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009]),\n",
       "   'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,  70211, 128009,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100])},\n",
       "  'idk': {'input_ids': tensor([128000,     58,   4386,    931,     11,    220,   4386,  11030,     11,\n",
       "              220,  22750,     20,     11,    220,   4386,  11194,     11,    220,\n",
       "            15828,     11,    220,  20062,   2287,     11,    220,   5894,     18,\n",
       "               11,    220,  10568,    914,     11,    220,  16955,     21,     11,\n",
       "              220,    914,     11,    220,  25136,     15,     11,    220,   8610,\n",
       "               11,    220,  14087,     21,     11,    220,    972,     11,    220,\n",
       "             3753,     11,    220,  10895,   1187,     11,    220,  16955,     21,\n",
       "               11,    220,    914,     11,    220,   8610,     11,    220,  10674,\n",
       "               22,     11,    220,   4278,   5495,     11,    220,   8610,     11,\n",
       "              220,  14087,     21,     11,    220,    777,     11,    220,  15828,\n",
       "               11,    220,   4386,  13858,     11,    220,   4386,  11030,     11,\n",
       "              220,  23213,     11,    220,   4386,  11194,     11,    220,  15828,\n",
       "               11,    220,  19695,     18,     11,    220,  22608,   3391,     11,\n",
       "              220,  23402,     11,    220,  22369,    508,     11,    220,  23033,\n",
       "             4643,     11,    220,  13655,     11,    220,   8190,     17,     11,\n",
       "              220,   5332,     11,    220,    966,     11,    220,   4386,  13858,\n",
       "               60,     40,   1541,    956,    617,    279,   4320,    311,    430,\n",
       "             3488,     13, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "           128009, 128009, 128009, 128009]),\n",
       "   'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,     40,   1541,    956,    617,    279,   4320,    311,    430,\n",
       "             3488,     13, 128009,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "             -100,   -100,   -100,   -100])}}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = dataset[0]\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00cfda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
