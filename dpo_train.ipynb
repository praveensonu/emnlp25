{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b938483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "from copy import deepcopy\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d29713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/praveen/theoden/emnlp_25/forget_20_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a24d1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"idk.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    idk_responses = [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee6e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chat_format(question, response):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "\n",
    "# Create new columns\n",
    "df[\"chosen\"] = df.apply(lambda row: make_chat_format(row[\"question\"], random.choice(idk_responses)), axis=1)\n",
    "df[\"rejected\"] = df.apply(lambda row: make_chat_format(row[\"question\"], row[\"answer\"]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9014f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"answer\", \"paraphrased_question\", \"wikipage\", \"title\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44c3a053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What nationality was Benedetto Varchi?'},\n",
       " {'role': 'assistant', 'content': 'I lack the specifics on that matter.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['chosen'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab73585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dpo_forget.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "715431c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "def extract_prompt_response_from_chat(chat_data: list, tokenizer: AutoTokenizer):\n",
    "    # ... (keep the function as defined before)\n",
    "    if not chat_data or not isinstance(chat_data[-1], dict) or chat_data[-1].get('role') != 'assistant': # Added type check\n",
    "        log.warning(f\"Chat data is not a list ending with an assistant dict: {chat_data}\")\n",
    "        return None, None\n",
    "    # ... rest of the function\n",
    "    final_response = chat_data[-1]['content']\n",
    "    prompt_chat = chat_data[:-1] # Everything before the last assistant message\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        prompt_chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return formatted_prompt, final_response\n",
    "# --- ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5385128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "import ast\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb2f6df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'praveensonu/llama_3_1_8b_finetuned'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63170aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Set pad token if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a407acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        # --- PARSE THE STRINGS into Python lists ---\n",
    "        # Use ast.literal_eval for safe parsing\n",
    "        chosen_chat_str = row['chosen']\n",
    "        rejected_chat_str = row['rejected']\n",
    "\n",
    "        # Ensure they are actually strings before trying to parse\n",
    "        if not isinstance(chosen_chat_str, str):\n",
    "             log.warning(f\"Row {index}: 'chosen' data is not a string, it's {type(chosen_chat_str)}. Skipping.\")\n",
    "             continue\n",
    "        if not isinstance(rejected_chat_str, str):\n",
    "             log.warning(f\"Row {index}: 'rejected' data is not a string, it's {type(rejected_chat_str)}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        chosen_chat_list = ast.literal_eval(chosen_chat_str)\n",
    "        rejected_chat_list = ast.literal_eval(rejected_chat_str)\n",
    "        # --- Parsing done ---\n",
    "\n",
    "        # Ensure the parsed result is a list (literal_eval can return other types too)\n",
    "        if not isinstance(chosen_chat_list, list):\n",
    "            log.warning(f\"Row {index}: Parsed 'chosen' is not a list ({type(chosen_chat_list)}). Skipping.\")\n",
    "            continue\n",
    "        if not isinstance(rejected_chat_list, list):\n",
    "            log.warning(f\"Row {index}: Parsed 'rejected' is not a list ({type(rejected_chat_list)}). Skipping.\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # --- Now pass the PARSED LISTS to the function ---\n",
    "        formatted_prompt_chosen, final_response_chosen = extract_prompt_response_from_chat(chosen_chat_list, tokenizer)\n",
    "        formatted_prompt_rejected, final_response_rejected = extract_prompt_response_from_chat(rejected_chat_list, tokenizer)\n",
    "        # --- ---\n",
    "\n",
    "        # --- Crucial Check ---\n",
    "        if formatted_prompt_chosen is None or formatted_prompt_rejected is None:\n",
    "            # The function already logged the specific reason\n",
    "            log.warning(f\"Skipping row {index} due to invalid chat format detected by extract function.\")\n",
    "            continue\n",
    "        if formatted_prompt_chosen != formatted_prompt_rejected:\n",
    "            log.warning(f\"Skipping row {index} due to mismatched prompts after formatting.\")\n",
    "            continue\n",
    "\n",
    "        processed_data.append({\n",
    "            'prompt': formatted_prompt_chosen,\n",
    "            'chosen': final_response_chosen,\n",
    "            'rejected': final_response_rejected\n",
    "        })\n",
    "\n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        # Catch errors during ast.literal_eval if the string is malformed\n",
    "        log.warning(f\"Skipping row {index} due to parsing error: {e}\")\n",
    "        log.warning(f\"  Problematic 'chosen' string: {row.get('chosen', 'N/A')}\")\n",
    "        log.warning(f\"  Problematic 'rejected' string: {row.get('rejected', 'N/A')}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected errors during processing for a row\n",
    "        log.error(f\"Unexpected error processing row {index}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da4d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 98 valid examples.\n",
      "Example processed data point:\n",
      "{'prompt': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat nationality was Benedetto Varchi?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', 'chosen': \"I'm not certain about that.\", 'rejected': 'Italian'}\n"
     ]
    }
   ],
   "source": [
    "if not processed_data:\n",
    "     raise ValueError(\"No valid data points found after processing the DataFrame!\")\n",
    "\n",
    "train_dataset = Dataset.from_list(processed_data)\n",
    "\n",
    "print(f\"Processed {len(train_dataset)} valid examples.\")\n",
    "if train_dataset:\n",
    "    print(\"Example processed data point:\")\n",
    "    print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04724882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_nll(model, inputs):\n",
    "    \"\"\"\n",
    "    Compute the negative log-likelihood for each sequence in a batch independently.\n",
    "    Args:\n",
    "        model: The model to compute NLL with.\n",
    "        inputs: A dictionary containing 'input_ids', 'attention_mask', and 'labels'.\n",
    "                'labels' are expected to be the same as 'input_ids' with padding tokens (-100).\n",
    "    Returns:\n",
    "        A tensor of shape (batch_size,) containing the NLL for each sequence,\n",
    "        and the model outputs.\n",
    "    \"\"\"\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    labels = inputs[\"labels\"]\n",
    "\n",
    "    # Shift so that tokens < n predict n\n",
    "    shifted_logits = logits[..., :-1, :].contiguous()\n",
    "    shifted_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    # Calculate per-token loss\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=-100, reduction='none')\n",
    "    # loss shape: (batch_size, seq_len - 1)\n",
    "    loss = loss_function(shifted_logits.transpose(-1, -2), shifted_labels)\n",
    "\n",
    "    # Sum loss over sequence dimension\n",
    "    # seq_nll shape: (batch_size,)\n",
    "    seq_nll = loss.sum(dim=-1)\n",
    "    return seq_nll, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e5b13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dpo_loss(policy_model,\n",
    "                     ref_model,\n",
    "                     win_inputs,\n",
    "                     lose_inputs,\n",
    "                     beta=0.1):\n",
    "    \"\"\"\n",
    "    Compute the DPO loss for a batch of paired examples.\n",
    "    Args:\n",
    "        policy_model: The model being trained.\n",
    "        ref_model: The frozen reference model.\n",
    "        win_inputs: Dictionary of inputs for the chosen responses.\n",
    "        lose_inputs: Dictionary of inputs for the rejected responses.\n",
    "        beta: The DPO temperature parameter.\n",
    "    Returns:\n",
    "        Scalar DPO loss, policy chosen outputs, policy rejected outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Policy model NLL calculations (requires gradients)\n",
    "    win_policy_nll, win_policy_outputs = compute_batch_nll(policy_model, win_inputs)\n",
    "    lose_policy_nll, lose_policy_outputs = compute_batch_nll(policy_model, lose_inputs)\n",
    "\n",
    "    # Reference model NLL calculations (no gradients needed)\n",
    "    with torch.no_grad():\n",
    "        win_ref_nll, _ = compute_batch_nll(ref_model, win_inputs)\n",
    "        lose_ref_nll, _ = compute_batch_nll(ref_model, lose_inputs)\n",
    "\n",
    "    # Convert NLL to Log Probabilities (log P = -NLL)\n",
    "    # Note: The signs are flipped compared to the original code's ratio calculation\n",
    "    # because we directly use the definition: log P_pi(y) - log P_ref(y)\n",
    "    win_logp_policy = -win_policy_nll\n",
    "    win_logp_ref = -win_ref_nll\n",
    "    lose_logp_policy = -lose_policy_nll\n",
    "    lose_logp_ref = -lose_ref_nll\n",
    "\n",
    "    # Calculate log ratios\n",
    "    pi_logratios = win_logp_policy - lose_logp_policy # log(pi_policy(win) / pi_policy(lose))\n",
    "    ref_logratios = win_logp_ref - lose_logp_ref    # log(pi_ref(win) / pi_ref(lose))\n",
    "\n",
    "    # DPO loss formula (standard version)\n",
    "    # log(sigmoid( beta * ( (logP_policy(win) - logP_ref(win)) - (logP_policy(lose) - logP_ref(lose)) ) ))\n",
    "    # = log(sigmoid( beta * ( (logP_policy(win) - logP_policy(lose)) - (logP_ref(win) - logP_ref(lose)) ) ))\n",
    "    # = log(sigmoid( beta * (pi_logratios - ref_logratios) ))\n",
    "    logits = beta * (pi_logratios - ref_logratios)\n",
    "    loss = -F.logsigmoid(logits).mean() # Average over the batch\n",
    "\n",
    "    return loss, win_policy_outputs, lose_policy_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca12280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDpoTrainer(Trainer):\n",
    "    def __init__(self, *args, ref_model: nn.Module = None, beta: float = 0.1, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if ref_model is None:\n",
    "            raise ValueError(\"ref_model must be provided for DPO training.\")\n",
    "\n",
    "        self.ref_model = ref_model\n",
    "        self.beta = beta\n",
    "\n",
    "        # Ensure the reference model is on the right device and in eval mode\n",
    "        # Trainer will handle device placement for the main 'model'\n",
    "        # Place ref_model on the same device *once*\n",
    "        # Note: If using FSDP or complex setups, this might need adjustment\n",
    "        if self.args.world_size > 1:\n",
    "             # If using DDP, ensure ref_model is on the correct local rank device\n",
    "             self.ref_model = self.ref_model.to(self.args.device)\n",
    "\n",
    "        self.ref_model.eval()\n",
    "\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By overriding this method, we implement DPO loss calculation.\n",
    "        \"\"\"\n",
    "        # Ensure ref_model is on the correct device (important if devices change or in specific setups)\n",
    "        # This might be slightly redundant if placed correctly in __init__, but safer.\n",
    "        if self.ref_model.device != model.device:\n",
    "             self.ref_model = self.ref_model.to(model.device)\n",
    "\n",
    "        # Prepare inputs for chosen and rejected responses\n",
    "        win_inputs = {\n",
    "            \"input_ids\": inputs[\"chosen_input_ids\"],\n",
    "            \"attention_mask\": inputs[\"chosen_attention_mask\"],\n",
    "            \"labels\": inputs[\"chosen_labels\"],\n",
    "        }\n",
    "        lose_inputs = {\n",
    "            \"input_ids\": inputs[\"rejected_input_ids\"],\n",
    "            \"attention_mask\": inputs[\"rejected_attention_mask\"],\n",
    "            \"labels\": inputs[\"rejected_labels\"],\n",
    "        }\n",
    "\n",
    "        # Call the DPO loss function\n",
    "        # 'model' is the policy model being trained (potentially DDP wrapped)\n",
    "        loss, win_outputs, lose_outputs = compute_dpo_loss(\n",
    "            policy_model=model,\n",
    "            ref_model=self.ref_model,\n",
    "            win_inputs=win_inputs,\n",
    "            lose_inputs=lose_inputs,\n",
    "            beta=self.beta\n",
    "        )\n",
    "\n",
    "        # If you need metrics or want to return outputs, handle them here\n",
    "        # For standard training, just returning the loss is sufficient\n",
    "        outputs = (win_outputs, lose_outputs) if return_outputs else None\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    # Optional: Override _save_snapshot, save_model etc. if you need to ensure\n",
    "    # the ref_model isn't saved or handled incorrectly during checkpointing.\n",
    "    # Usually, Trainer saves the 'model' attribute, which is the policy model,\n",
    "    # so the ref_model shouldn't interfere unless you modify save logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e59729e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class ChatDPODataCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None # Max length for the *entire* sequence (prompt + response)\n",
    "    max_prompt_length: Optional[int] = None # Optional: Max length for the prompt part\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        batch = {}\n",
    "        chosen_batch = []\n",
    "        rejected_batch = []\n",
    "\n",
    "        if not features:\n",
    "            return batch\n",
    "\n",
    "        for feature in features:\n",
    "            prompt = feature['prompt'] # This is the already formatted prompt string\n",
    "            chosen = feature['chosen']\n",
    "            rejected = feature['rejected']\n",
    "\n",
    "            # Tokenize prompt (without padding/truncation yet)\n",
    "            prompt_tokens = self.tokenizer(prompt, add_special_tokens=False) # Assuming template added special tokens\n",
    "            # Apply prompt truncation if specified\n",
    "            if self.max_prompt_length:\n",
    "                prompt_tokens = {k: v[-self.max_prompt_length:] for k, v in prompt_tokens.items()}\n",
    "\n",
    "\n",
    "            # Tokenize responses, adding EOS token\n",
    "            # Important: Check if your tokenizer/chat template *already* adds EOS after assistant response\n",
    "            # If it does, you might not need `+ self.tokenizer.eos_token` here. Test this!\n",
    "            chosen_tokens = self.tokenizer(chosen + self.tokenizer.eos_token, add_special_tokens=False)\n",
    "            rejected_tokens = self.tokenizer(rejected + self.tokenizer.eos_token, add_special_tokens=False)\n",
    "\n",
    "            # --- Combine prompt and response tokens ---\n",
    "            chosen_sequence = {\n",
    "                'input_ids': prompt_tokens['input_ids'] + chosen_tokens['input_ids'],\n",
    "                'attention_mask': prompt_tokens['attention_mask'] + chosen_tokens['attention_mask']\n",
    "            }\n",
    "            rejected_sequence = {\n",
    "                'input_ids': prompt_tokens['input_ids'] + rejected_tokens['input_ids'],\n",
    "                'attention_mask': prompt_tokens['attention_mask'] + rejected_tokens['attention_mask']\n",
    "            }\n",
    "\n",
    "            # --- Create Labels (Mask Prompt Tokens) ---\n",
    "            chosen_labels = [-100] * len(prompt_tokens['input_ids']) + chosen_tokens['input_ids']\n",
    "            rejected_labels = [-100] * len(prompt_tokens['input_ids']) + rejected_tokens['input_ids']\n",
    "\n",
    "            # --- Apply Max Length Truncation (Entire Sequence) ---\n",
    "            if self.max_length:\n",
    "                for seq in [chosen_sequence, rejected_sequence]:\n",
    "                     for key in ['input_ids', 'attention_mask']:\n",
    "                         seq[key] = seq[key][:self.max_length]\n",
    "                chosen_labels = chosen_labels[:self.max_length]\n",
    "                rejected_labels = rejected_labels[:self.max_length]\n",
    "\n",
    "\n",
    "            chosen_batch.append({**chosen_sequence, 'labels': chosen_labels})\n",
    "            rejected_batch.append({**rejected_sequence, 'labels': rejected_labels})\n",
    "\n",
    "\n",
    "        # --- Padding ---\n",
    "        # Pad the chosen sequences\n",
    "        padded_chosen = self.tokenizer.pad(\n",
    "            chosen_batch,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length, # Use max_length for padding\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Pad the rejected sequences\n",
    "        padded_rejected = self.tokenizer.pad(\n",
    "            rejected_batch,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Reconstruct the batch keys expected by the CustomDpoTrainer\n",
    "        batch['chosen_input_ids'] = padded_chosen['input_ids']\n",
    "        batch['chosen_attention_mask'] = padded_chosen['attention_mask']\n",
    "        batch['chosen_labels'] = padded_chosen['labels']\n",
    "        batch['rejected_input_ids'] = padded_rejected['input_ids']\n",
    "        batch['rejected_attention_mask'] = padded_rejected['attention_mask']\n",
    "        batch['rejected_labels'] = padded_rejected['labels']\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "265ed7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edad1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = ChatDPODataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024, # Example: Max sequence length\n",
    "    max_prompt_length=512, # Example: Max prompt length\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1685f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = cfg.model_id # Or your base SFT model path\n",
    "output_dir = \"./dpo_custom_trainer_output\"\n",
    "beta = 0.1 # DPO hyperparameter\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 1\n",
    "batch_size_per_device = 2 # Adjust based on GPU memory\n",
    "gradient_accumulation_steps = 4\n",
    "max_length = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67b3cb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0183db5cd026443ea78e449af3cb2a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d476812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_model = deepcopy(policy_model)\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "ref_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ada8360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size_per_device,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_epochs,\n",
    "    # DDP specific args handled by accelerate launch (or defaults)\n",
    "    # remove_unused_columns=False, # Important if your dataset has extra columns\n",
    "    logging_steps=10,\n",
    "    save_steps=100, # Adjust as needed\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\", # Or \"wandb\", \"tensorboard\"\n",
    "    # Add other arguments like weight_decay, warmup_steps etc. as needed\n",
    "    bf16=True, # Enable if your hardware supports it and you want faster training\n",
    "    # fp16=True, # Use instead of bf16 if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04602e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = ChatDPODataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024, # Example: Max sequence length\n",
    "    max_prompt_length=512, # Example: Max prompt length\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomDpoTrainer(\n",
    "    model=policy_model,       # The model to be trained (policy)\n",
    "    ref_model=ref_model,      # The frozen reference model\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=eval_dataset, # Optional\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    beta=beta,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
