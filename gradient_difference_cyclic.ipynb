{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf685de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72b0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from config import Config\n",
    "from peft import  LoraConfig, get_peft_model\n",
    "from data_module import DualDataset\n",
    "from collators import custom_gd_collator_forget\n",
    "from utils import find_all_linear_names\n",
    "from forget_trainer import GradDiffTrainer\n",
    "from accelerate import Accelerator\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b220fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb505269",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.save_dir = 'outputs/wpu_cyclic_grad_diff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d911c1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the paths to forget, retain and test set\n"
     ]
    }
   ],
   "source": [
    "print('loading the paths to forget, retain and test set')\n",
    "forget = pd.read_csv(cfg.forget_path) #cfg.forget_path\n",
    "retain = pd.read_csv(cfg.retain_path) #cfg.retain_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c69a45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the Tokenizer praveensonu/llama_3_1_8b_finetuned\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading the Tokenizer {cfg.model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_id, token = cfg.access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e4cf52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the Model praveensonu/llama_3_1_8b_finetuned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea2802e647c4af2a4c6d24738da53ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"\\nLoading the Model {cfg.model_id}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.model_id, \n",
    "                                             torch_dtype = torch.bfloat16, \n",
    "                                             token=cfg.access_token,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d1cf794",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "        r = cfg.LoRA_r,\n",
    "        lora_alpha = cfg.LoRA_alpha,\n",
    "        lora_dropout= cfg.LoRA_dropout,\n",
    "        target_modules = find_all_linear_names(model),\n",
    "        bias = 'none',\n",
    "        task_type = 'CAUSAL_LM',\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154de072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "#model.generation_config.do_sample = True\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74bfff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset =  DualDataset(\n",
    "    forget_data = forget,\n",
    "    retain_data = retain,\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3041db8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1801\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89730443",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir = cfg.save_dir,\n",
    "        overwrite_output_dir= True,\n",
    "        learning_rate = cfg.lr,\n",
    "        per_device_train_batch_size= 4, \n",
    "        num_train_epochs= 10,\n",
    "        weight_decay = cfg.weight_decay,\n",
    "        logging_dir = f'{cfg.save_dir}/logs',\n",
    "        eval_strategy= 'no',\n",
    "        label_names = ['labels'],\n",
    "        bf16 = True,\n",
    "        gradient_accumulation_steps= 2,\n",
    "        #save_only_model=True,\n",
    "        report_to = 'wandb',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43620c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1458912/1030816357.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `GradDiffTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = GradDiffTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = GradDiffTrainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_dataset,\n",
    "        tokenizer = tokenizer,\n",
    "        data_collator = custom_gd_collator_forget,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d788c85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpraveenbushipaka942\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/praveen/theoden/emnlp_25/wandb/run-20250520_155556-c696xa3i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/praveenbushipaka942/huggingface/runs/c696xa3i' target=\"_blank\">outputs/wpu_cyclic_grad_diff</a></strong> to <a href='https://wandb.ai/praveenbushipaka942/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/praveenbushipaka942/huggingface' target=\"_blank\">https://wandb.ai/praveenbushipaka942/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/praveenbushipaka942/huggingface/runs/c696xa3i' target=\"_blank\">https://wandb.ai/praveenbushipaka942/huggingface/runs/c696xa3i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 50:55, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>-177.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>-292.813800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>-312.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>-344.932500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=-291.0765625, metrics={'train_runtime': 3059.4876, 'train_samples_per_second': 5.887, 'train_steps_per_second': 0.735, 'total_flos': 0.0, 'train_loss': -291.0765625, 'epoch': 9.977827050997783})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46b1a2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forget LoRA adapter saved at outputs/wpu_cyclic_grad_diff\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('outputs/wpu_cyclic_grad_diff/tokenizer_config.json',\n",
       " 'outputs/wpu_cyclic_grad_diff/special_tokens_map.json',\n",
       " 'outputs/wpu_cyclic_grad_diff/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'\\nForget LoRA adapter saved at {cfg.save_dir}')\n",
    "model.save_pretrained(cfg.save_dir)\n",
    "tokenizer.save_pretrained(cfg.save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
