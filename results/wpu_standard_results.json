{
    "grad_diff": {
        "forget_efficacy": 0.8991434321276286,
        "model_utility": 0.26824510506837856,
        "forget_scores": [
            0.0,
            0.0,
            0.3025697036171142
        ],
        "retain_scores": [
            3.800240518062102e-05,
            0.3131466257902612,
            0.49155068700969384
        ],
        "qa_perplexity_forget": 1e+308,
        "average_loss_forget": 191.11209228515625,
        "perp_num_batches_forget": 25,
        "qa_perplexity_retain": 4.846750259399414,
        "average_loss_retain": 1.5783084899514586,
        "perp_num_batches_retain": 91,
        "exp_type": "standard",
        "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "batch_size": 4,
        "num_epochs": 10,
        "lr": 2e-05,
        "weight_decay": 0.01,
        "LoRA_r": 8,
        "LoRA_alpha": 32
    }
}