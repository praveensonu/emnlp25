{
    "grad_diff": {
        "forget_efficacy": 0.9827246765606106,
        "model_utility": 0.09501679743846246,
        "forget_scores": [
            0.0,
            0.0005102040816326531,
            0.05131576623653575
        ],
        "retain_scores": [
            0.03672307771430071,
            0.38872980866418744,
            0.5649555546056101
        ],
        "qa_perplexity_forget": 1e+308,
        "average_loss_forget": 128.46600952148438,
        "perp_num_batches_forget": 25,
        "qa_perplexity_retain": 4.987115859985352,
        "average_loss_retain": 1.6068577252585312,
        "perp_num_batches_retain": 87,
        "exp_type": "retain_mean",
        "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "batch_size": 4,
        "num_epochs": 10,
        "lr": 2e-05,
        "weight_decay": 0.01,
        "LoRA_r": 8,
        "LoRA_alpha": 32
    },
    "grad_ascent": {
        "forget_efficacy": 0.9694999335739178,
        "model_utility": 0.0,
        "forget_scores": [
            0.0,
            0.0005102040816326531,
            0.09098999519661373
        ],
        "retain_scores": [
            0.0,
            0.00014492753623188405,
            0.06168554049741099
        ],
        "qa_perplexity_forget": 1e+308,
        "average_loss_forget": 130.21407958984375,
        "perp_num_batches_forget": 25,
        "qa_perplexity_retain": 1e+308,
        "average_loss_retain": 117.21785218688264,
        "perp_num_batches_retain": 87,
        "exp_type": "retain_mean",
        "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "batch_size": 4,
        "num_epochs": 10,
        "lr": 2e-05,
        "weight_decay": 0.01,
        "LoRA_r": 8,
        "LoRA_alpha": 32
    }
}