{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1479a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ad63e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import re\n",
    "from config import Config\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e886301",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'praveensonu/llama_3_1_8b_finetuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03b8358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Loading Tokenizer -----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ace6f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'test': 'all/test-00000-of-00001.parquet', 'validation': 'all/validation-00000-of-00001.parquet', 'dev': 'all/dev-00000-of-00001.parquet', 'auxiliary_train': 'all/auxiliary_train-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/cais/mmlu/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62931bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1898087",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.loss_type = 'dpo'\n",
    "cfg.save_dir = '/home/praveen/theoden/emnlp25/outputs/dpo_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b99d31f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo\n"
     ]
    }
   ],
   "source": [
    "#cfg.loss_type = 'pre_unlearning'\n",
    "print(cfg.loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5472714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading peft model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba9b646cd2747208a817cf84e0a07cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('loading peft model')\n",
    "base_model = AutoModelForCausalLM.from_pretrained(cfg.model_id, token = cfg.access_token, device_map = \"auto\", torch_dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(base_model, cfg.save_dir, device_map=\"auto\", torch_dtype=torch.bfloat16) \n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3087fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = pd.read_csv('mmlu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90e47625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(question, choices):\n",
    "    # force a standard Python list so we get commas in the printed repr\n",
    "    choices = list(choices)\n",
    "    return f\"\"\"You are a multiple-choice QA assistant.\n",
    "Given a question and exactly four answer choices labeled A, B, C, and D, reply with **only** the single letter of the correct answer (A, B, C, or D), and nothing else.\n",
    "\n",
    "Question: {question}\n",
    "A) {choices[0]}\n",
    "B) {choices[1]}\n",
    "C) {choices[2]}\n",
    "D) {choices[3]}\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3b4359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt, tokenizer, model):\n",
    "    with torch.no_grad():\n",
    "        ips = tokenizer(prompt, return_tensors='pt', padding=True).to('cuda')\n",
    "        out = model.generate(\n",
    "            **ips,\n",
    "            max_new_tokens=10,     \n",
    "            do_sample=False,\n",
    "            eos_token_id = tokenizer.eos_token_id,\n",
    "            pad_token_id = tokenizer.pad_token_id\n",
    "        )\n",
    "        return tokenizer.decode(out[0], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d06aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_letter(prompt, tokenizer, model):\n",
    "    raw = generate_answer(prompt, tokenizer, model)      \n",
    "    m = re.search(r'([A-D])\\s*$', raw.strip())\n",
    "    return m.group(1) if m else raw.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15e54a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b20f9204f14448ad71033f66435866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praveen/miniconda3/envs/emnlp/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/praveen/miniconda3/envs/emnlp/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def answer_for_row(row):\n",
    "    prompt = make_prompt(row['question'], row['choices'])\n",
    "    return generate_answer_letter(prompt, tokenizer, model)\n",
    "\n",
    "df['temp_answers'] = df.progress_apply(answer_for_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d3878f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_letter = {1: 'A', 2: 'B', 3: 'C', 4: 'D'}\n",
    "df['answer_letter'] = df['answer'].map(num_to_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42ee881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cfg.loss_type] = df['temp_answers'].str.extract(r'Answer:\\s*([A-D])', expand=False).str.strip()\n",
    "df[cfg.loss_type] = df[cfg.loss_type].fillna(df['temp_answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e9ca16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>subject</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>temp_answers</th>\n",
       "      <th>answer_letter</th>\n",
       "      <th>dpo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Find the degree for the given field extension ...</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>[0, 4, 2, 6]</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the i...</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>[8, 2, 24, 120]</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find all zeros in the indicated finite field o...</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>[0, 1, 0,1, 0,4]</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Statement 1 | A factor group of a non-Abelian ...</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>[True, True, False, False, True, False, False,...</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Find the product of the given polynomials in t...</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>[2x^2 + 5, 6x^2 + 4x + 6, 0, x^2 + 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question           subject  \\\n",
       "0  Find the degree for the given field extension ...  abstract_algebra   \n",
       "1  Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the i...  abstract_algebra   \n",
       "2  Find all zeros in the indicated finite field o...  abstract_algebra   \n",
       "3  Statement 1 | A factor group of a non-Abelian ...  abstract_algebra   \n",
       "4  Find the product of the given polynomials in t...  abstract_algebra   \n",
       "\n",
       "                                             choices  answer temp_answers  \\\n",
       "0                                       [0, 4, 2, 6]       1            B   \n",
       "1                                    [8, 2, 24, 120]       2            B   \n",
       "2                                   [0, 1, 0,1, 0,4]       3            D   \n",
       "3  [True, True, False, False, True, False, False,...       1            C   \n",
       "4              [2x^2 + 5, 6x^2 + 4x + 6, 0, x^2 + 1]       1            B   \n",
       "\n",
       "  answer_letter dpo  \n",
       "0             A   B  \n",
       "1             B   B  \n",
       "2             C   D  \n",
       "3             A   C  \n",
       "4             A   B  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "759227fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dpo\n",
       "A    3696\n",
       "C    3632\n",
       "B    3596\n",
       "D    3118\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[cfg.loss_type].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9fd39fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 12.21%\n"
     ]
    }
   ],
   "source": [
    "df['correct'] = df[cfg.loss_type] == df['answer_letter']\n",
    "accuracy = df['correct'].mean()\n",
    "print(f'Accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4157c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_list = df[cfg.loss_type].tolist()\n",
    "answer[cfg.loss_type] = answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "840123f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 12.21%\n"
     ]
    }
   ],
   "source": [
    "answer['correct'] = answer[cfg.loss_type] == answer['answer_letter']\n",
    "accuracy = answer['correct'].mean()\n",
    "print(f'Accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0312bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>subject</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_letter</th>\n",
       "      <th>pre_unlearning</th>\n",
       "      <th>gd_balanced</th>\n",
       "      <th>correct</th>\n",
       "      <th>gd_direct</th>\n",
       "      <th>gd_indirect</th>\n",
       "      <th>...</th>\n",
       "      <th>dpo_balanced</th>\n",
       "      <th>dpo_cyclic</th>\n",
       "      <th>dpo_melu</th>\n",
       "      <th>npo_retain</th>\n",
       "      <th>npo_direct</th>\n",
       "      <th>npo_indirect</th>\n",
       "      <th>npo_balanced</th>\n",
       "      <th>npo_cyclic</th>\n",
       "      <th>npo_melu</th>\n",
       "      <th>dpo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Find the degree for the given field extension ...</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>['0' '4' '2' '6']</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>You are a multiple-choice QA assistant.\\nGiven...</td>\n",
       "      <td>...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the i...</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>['8' '2' '24' '120']</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>True</td>\n",
       "      <td>B</td>\n",
       "      <td>You are a multiple-choice QA assistant.\\nGiven...</td>\n",
       "      <td>...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find all zeros in the indicated finite field o...</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>['0' '1' '0,1' '0,4']</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>False</td>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Statement 1 | A factor group of a non-Abelian ...</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>['True, True' 'False, False' 'True, False' 'Fa...</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>You are a multiple-choice QA assistant.\\nGiven...</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Find the product of the given polynomials in t...</td>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>['2x^2 + 5' '6x^2 + 4x + 6' '0' 'x^2 + 1']</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>You are a multiple-choice QA assistant.\\nGiven...</td>\n",
       "      <td>...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question           subject  \\\n",
       "0  Find the degree for the given field extension ...  abstract_algebra   \n",
       "1  Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the i...  abstract_algebra   \n",
       "2  Find all zeros in the indicated finite field o...  abstract_algebra   \n",
       "3  Statement 1 | A factor group of a non-Abelian ...  abstract_algebra   \n",
       "4  Find the product of the given polynomials in t...  abstract_algebra   \n",
       "\n",
       "                                             choices  answer answer_letter  \\\n",
       "0                                  ['0' '4' '2' '6']       1             A   \n",
       "1                               ['8' '2' '24' '120']       2             B   \n",
       "2                              ['0' '1' '0,1' '0,4']       3             C   \n",
       "3  ['True, True' 'False, False' 'True, False' 'Fa...       1             A   \n",
       "4         ['2x^2 + 5' '6x^2 + 4x + 6' '0' 'x^2 + 1']       1             A   \n",
       "\n",
       "  pre_unlearning gd_balanced  correct gd_direct  \\\n",
       "0              B           D    False         B   \n",
       "1              B           B     True         B   \n",
       "2              D           D    False         D   \n",
       "3              C           C    False         C   \n",
       "4              B           B    False         B   \n",
       "\n",
       "                                         gd_indirect  ... dpo_balanced  \\\n",
       "0  You are a multiple-choice QA assistant.\\nGiven...  ...            B   \n",
       "1  You are a multiple-choice QA assistant.\\nGiven...  ...            B   \n",
       "2                                                  C  ...            D   \n",
       "3  You are a multiple-choice QA assistant.\\nGiven...  ...            C   \n",
       "4  You are a multiple-choice QA assistant.\\nGiven...  ...            B   \n",
       "\n",
       "  dpo_cyclic dpo_melu npo_retain npo_direct npo_indirect npo_balanced  \\\n",
       "0          B        B          B          B            B            B   \n",
       "1          B        B          B          B            B            B   \n",
       "2          D        D          D          D            D            D   \n",
       "3          C        C          C          C            C            C   \n",
       "4          B        B          B          B            B            B   \n",
       "\n",
       "  npo_cyclic npo_melu dpo  \n",
       "0          B        B   B  \n",
       "1          B        B   B  \n",
       "2          D        D   D  \n",
       "3          C        C   C  \n",
       "4          B        B   B  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df78eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.to_csv('mmlu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2912285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad82644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = pd.read_csv('mmlu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02fee120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'subject', 'choices', 'answer', 'answer_letter',\n",
       "       'pre_unlearning', 'gd_balanced', 'correct', 'gd_direct', 'gd_indirect',\n",
       "       'gd_cyclic', 'gd_melu', 'gd_ascent', 'gd_diff', 'npo', 'dpo_retain',\n",
       "       'dpo_direct', 'dpo_indirect', 'dpo_balanced', 'dpo_cyclic', 'dpo_melu',\n",
       "       'npo_retain', 'npo_direct', 'npo_indirect', 'npo_balanced',\n",
       "       'npo_cyclic', 'npo_melu', 'dpo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d35a10e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12412761714855433\n",
      "0.09543710925322503\n",
      "0.09690062524055489\n",
      "0.09607397323432638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praveen/miniconda3/envs/emnlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/praveen/miniconda3/envs/emnlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "answers_og = answer['answer_letter'].tolist()\n",
    "pre = answer['pre_unlearning'].tolist()\n",
    "\n",
    "print(accuracy_score(answers_og, pre))\n",
    "print(precision_score(answers_og, pre, average='macro'))\n",
    "print(recall_score(answers_og, pre, average='macro'))\n",
    "print(f1_score(answers_og, pre, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e85b9b",
   "metadata": {},
   "source": [
    "### lmharness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d2755",
   "metadata": {},
   "source": [
    "### with deep eval\n",
    "\n",
    "- got 0.0 MMLU benchmark score on pre unlearning model, there might be a problem in the deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8725b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreUnlearnedLLM(DeepEvalBaseLLM):\n",
    "    def __init__(self, model, tokenizer, model_id):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_id = model_id\n",
    "    \n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def generate(self, prompt:str) -> str:\n",
    "        model = self.load_model()\n",
    "\n",
    "        device = 'cuda'\n",
    "\n",
    "        model_ips = self.tokenizer([prompt], return_tensors = 'pt', padding=True).to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(**model_ips, max_new_tokens = 100, do_sample = True)\n",
    "        return self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    async def a_generate(self, prompt:str) -> str:\n",
    "        return self.generate(prompt)\n",
    "    \n",
    "    def batch_generate(self, prompts: List[str]) -> List[str]:\n",
    "        model = self.load_model()\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    def get_model_name(self):\n",
    "        return self.model_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_unlearned = PreUnlearnedLLM(model, tokenizer, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb849e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = MMLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf012a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing high_school_european_history (batch_size=5):   0%|          | 0/33 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):   3%|▎         | 1/33 [00:03<01:40,  3.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):   6%|▌         | 2/33 [00:06<01:33,  3.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):   9%|▉         | 3/33 [00:08<01:29,  2.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  12%|█▏        | 4/33 [00:11<01:26,  2.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  15%|█▌        | 5/33 [00:14<01:22,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  18%|█▊        | 6/33 [00:17<01:19,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  21%|██        | 7/33 [00:21<01:21,  3.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  24%|██▍       | 8/33 [00:24<01:17,  3.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  27%|██▋       | 9/33 [00:27<01:12,  3.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  30%|███       | 10/33 [00:30<01:09,  3.01s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  33%|███▎      | 11/33 [00:33<01:04,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  36%|███▋      | 12/33 [00:35<01:01,  2.91s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  39%|███▉      | 13/33 [00:38<00:57,  2.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  42%|████▏     | 14/33 [00:41<00:55,  2.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  45%|████▌     | 15/33 [00:44<00:52,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  48%|████▊     | 16/33 [00:47<00:49,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  52%|█████▏    | 17/33 [00:50<00:46,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  55%|█████▍    | 18/33 [00:53<00:43,  2.91s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  58%|█████▊    | 19/33 [00:56<00:41,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  61%|██████    | 20/33 [00:59<00:39,  3.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  64%|██████▎   | 21/33 [01:02<00:35,  2.99s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  67%|██████▋   | 22/33 [01:05<00:33,  3.01s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  70%|██████▉   | 23/33 [01:08<00:30,  3.01s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  73%|███████▎  | 24/33 [01:11<00:26,  2.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  76%|███████▌  | 25/33 [01:14<00:23,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  79%|███████▉  | 26/33 [01:17<00:20,  2.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  82%|████████▏ | 27/33 [01:20<00:17,  2.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  85%|████████▍ | 28/33 [01:22<00:13,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  88%|████████▊ | 29/33 [01:25<00:11,  2.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  91%|█████████ | 30/33 [01:28<00:08,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  94%|█████████▍| 31/33 [01:31<00:05,  2.88s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5):  97%|█████████▋| 32/33 [01:34<00:02,  2.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_european_history (batch_size=5): 100%|██████████| 33/33 [01:37<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=high_school_european_history): 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing business_ethics (batch_size=5):   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):   5%|▌         | 1/20 [00:02<00:49,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  10%|█         | 2/20 [00:05<00:46,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  15%|█▌        | 3/20 [00:07<00:44,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  20%|██        | 4/20 [00:10<00:41,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  25%|██▌       | 5/20 [00:12<00:38,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  30%|███       | 6/20 [00:15<00:35,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  35%|███▌      | 7/20 [00:18<00:33,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  40%|████      | 8/20 [00:20<00:30,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  45%|████▌     | 9/20 [00:23<00:28,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  50%|█████     | 10/20 [00:25<00:25,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  55%|█████▌    | 11/20 [00:28<00:23,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  60%|██████    | 12/20 [00:31<00:20,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  65%|██████▌   | 13/20 [00:33<00:18,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  70%|███████   | 14/20 [00:36<00:15,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  75%|███████▌  | 15/20 [00:38<00:13,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  80%|████████  | 16/20 [00:41<00:10,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  85%|████████▌ | 17/20 [00:44<00:07,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  90%|█████████ | 18/20 [00:46<00:05,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5):  95%|█████████▌| 19/20 [00:49<00:02,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing business_ethics (batch_size=5): 100%|██████████| 20/20 [00:51<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=business_ethics): 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing clinical_knowledge (batch_size=5):   0%|          | 0/53 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):   2%|▏         | 1/53 [00:02<02:15,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):   4%|▍         | 2/53 [00:04<01:43,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):   6%|▌         | 3/53 [00:06<01:55,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):   8%|▊         | 4/53 [00:09<01:58,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):   9%|▉         | 5/53 [00:12<01:58,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  11%|█▏        | 6/53 [00:14<01:58,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  13%|█▎        | 7/53 [00:17<01:56,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  15%|█▌        | 8/53 [00:19<01:54,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  17%|█▋        | 9/53 [00:22<01:52,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  19%|█▉        | 10/53 [00:24<01:50,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  21%|██        | 11/53 [00:27<01:46,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  23%|██▎       | 12/53 [00:30<01:45,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  25%|██▍       | 13/53 [00:32<01:44,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  26%|██▋       | 14/53 [00:35<01:41,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  28%|██▊       | 15/53 [00:37<01:37,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  30%|███       | 16/53 [00:40<01:36,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  32%|███▏      | 17/53 [00:43<01:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  34%|███▍      | 18/53 [00:45<01:30,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  36%|███▌      | 19/53 [00:48<01:27,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  38%|███▊      | 20/53 [00:50<01:25,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  40%|███▉      | 21/53 [00:53<01:22,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  42%|████▏     | 22/53 [00:56<01:20,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  43%|████▎     | 23/53 [00:58<01:18,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  45%|████▌     | 24/53 [01:01<01:15,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  47%|████▋     | 25/53 [01:03<01:12,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  49%|████▉     | 26/53 [01:06<01:08,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  51%|█████     | 27/53 [01:08<01:06,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  53%|█████▎    | 28/53 [01:11<01:04,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  55%|█████▍    | 29/53 [01:12<00:54,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  57%|█████▋    | 30/53 [01:14<00:48,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  58%|█████▊    | 31/53 [01:16<00:42,  1.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  60%|██████    | 32/53 [01:17<00:39,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  62%|██████▏   | 33/53 [01:20<00:41,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  64%|██████▍   | 34/53 [01:23<00:42,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  66%|██████▌   | 35/53 [01:25<00:42,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  68%|██████▊   | 36/53 [01:28<00:41,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  70%|██████▉   | 37/53 [01:30<00:39,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  72%|███████▏  | 38/53 [01:33<00:37,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  74%|███████▎  | 39/53 [01:36<00:35,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  75%|███████▌  | 40/53 [01:38<00:33,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  77%|███████▋  | 41/53 [01:41<00:30,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  79%|███████▉  | 42/53 [01:43<00:28,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  81%|████████  | 43/53 [01:46<00:25,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  83%|████████▎ | 44/53 [01:49<00:23,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  85%|████████▍ | 45/53 [01:51<00:20,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  87%|████████▋ | 46/53 [01:54<00:18,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  89%|████████▊ | 47/53 [01:56<00:15,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  91%|█████████ | 48/53 [01:59<00:12,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  92%|█████████▏| 49/53 [02:02<00:10,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  94%|█████████▍| 50/53 [02:04<00:07,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  96%|█████████▌| 51/53 [02:07<00:05,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5):  98%|█████████▊| 52/53 [02:09<00:02,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing clinical_knowledge (batch_size=5): 100%|██████████| 53/53 [02:12<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=clinical_knowledge): 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing medical_genetics (batch_size=5):   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):   5%|▌         | 1/20 [00:02<00:49,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  10%|█         | 2/20 [00:05<00:46,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  15%|█▌        | 3/20 [00:07<00:43,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  20%|██        | 4/20 [00:10<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  25%|██▌       | 5/20 [00:12<00:38,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  30%|███       | 6/20 [00:15<00:36,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  35%|███▌      | 7/20 [00:18<00:33,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  40%|████      | 8/20 [00:20<00:31,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  45%|████▌     | 9/20 [00:23<00:28,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  50%|█████     | 10/20 [00:25<00:25,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  55%|█████▌    | 11/20 [00:28<00:23,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  60%|██████    | 12/20 [00:31<00:20,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  65%|██████▌   | 13/20 [00:33<00:17,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  70%|███████   | 14/20 [00:36<00:15,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  75%|███████▌  | 15/20 [00:38<00:12,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  80%|████████  | 16/20 [00:41<00:10,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  85%|████████▌ | 17/20 [00:43<00:07,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  90%|█████████ | 18/20 [00:46<00:05,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5):  95%|█████████▌| 19/20 [00:48<00:02,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing medical_genetics (batch_size=5): 100%|██████████| 20/20 [00:51<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=medical_genetics): 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing high_school_us_history (batch_size=5):   0%|          | 0/41 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):   2%|▏         | 1/41 [00:02<01:54,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):   5%|▍         | 2/41 [00:05<01:50,  2.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):   7%|▋         | 3/41 [00:08<01:47,  2.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  10%|▉         | 4/41 [00:11<01:45,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  12%|█▏        | 5/41 [00:13<01:30,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  15%|█▍        | 6/41 [00:15<01:19,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  17%|█▋        | 7/41 [00:18<01:24,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  20%|█▉        | 8/41 [00:20<01:26,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  22%|██▏       | 9/41 [00:23<01:26,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  24%|██▍       | 10/41 [00:26<01:25,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  27%|██▋       | 11/41 [00:29<01:22,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  29%|██▉       | 12/41 [00:32<01:21,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  32%|███▏      | 13/41 [00:35<01:19,  2.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  34%|███▍      | 14/41 [00:38<01:17,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  37%|███▋      | 15/41 [00:41<01:14,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  39%|███▉      | 16/41 [00:43<01:11,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  41%|████▏     | 17/41 [00:46<01:08,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  44%|████▍     | 18/41 [00:49<01:05,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  46%|████▋     | 19/41 [00:52<01:02,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  49%|████▉     | 20/41 [00:55<00:59,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  51%|█████     | 21/41 [00:58<00:57,  2.88s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  54%|█████▎    | 22/41 [01:00<00:49,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  56%|█████▌    | 23/41 [01:03<00:48,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  59%|█████▊    | 24/41 [01:05<00:46,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  61%|██████    | 25/41 [01:08<00:44,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  63%|██████▎   | 26/41 [01:11<00:42,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Batch Processing high_school_us_history (batch_size=5):  63%|██████▎   | 26/41 [01:13<00:42,  2.82s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/emnlp/lib/python3.11/site-packages/deepeval/benchmarks/mmlu/mmlu.py:215\u001b[0m, in \u001b[0;36mMMLU.batch_predict\u001b[0;34m(self, model, task, goldens)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m     responses: List[MultipleChoiceSchema] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschemas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mMultipleChoiceSchema\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(responses, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[0;31mTypeError\u001b[0m: PreUnlearnedLLM.batch_generate() got an unexpected keyword argument 'schemas'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_unlearned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/emnlp/lib/python3.11/site-packages/deepeval/benchmarks/mmlu/mmlu.py:75\u001b[0m, in \u001b[0;36mMMLU.evaluate\u001b[0;34m(self, model, batch_size)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(goldens), batch_size),\n\u001b[1;32m     72\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     73\u001b[0m ):\n\u001b[1;32m     74\u001b[0m     goldens_batch \u001b[38;5;241m=\u001b[39m goldens[i : i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m---> 75\u001b[0m     batch_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoldens_batch\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m golden, prediction_dict \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m     79\u001b[0m         goldens_batch, batch_predictions\n\u001b[1;32m     80\u001b[0m     ):\n\u001b[1;32m     81\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m prediction_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/emnlp/lib/python3.11/site-packages/deepeval/benchmarks/mmlu/mmlu.py:228\u001b[0m, in \u001b[0;36mMMLU.batch_predict\u001b[0;34m(self, model, task, goldens)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    224\u001b[0m         prompt\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutput \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Full answer not needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts\n\u001b[1;32m    227\u001b[0m     ]\n\u001b[0;32m--> 228\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predictions) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(goldens):\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom `batch_generate` method did not return the same number of generations as the number of prompts.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[28], line 31\u001b[0m, in \u001b[0;36mPreUnlearnedLLM.batch_generate\u001b[0;34m(self, prompts)\u001b[0m\n\u001b[1;32m     28\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(prompts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 31\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/emnlp/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/emnlp/lib/python3.11/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/emnlp/lib/python3.11/site-packages/transformers/generation/utils.py:3195\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3192\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3193\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[0;32m-> 3195\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(\n\u001b[1;32m   3196\u001b[0m     this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice, cur_len\u001b[38;5;241m=\u001b[39mcur_len, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m   3197\u001b[0m ):\n\u001b[1;32m   3198\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3199\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3201\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = benchmark.evaluate(model=pre_unlearned, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c4afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Score:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Score: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d4380c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
