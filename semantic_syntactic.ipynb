{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0wBzxr5AZmmR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/praveen/miniconda3/envs/emnlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from similarities import print_similarities, compare_sentence_lists\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from config import Config\n",
        "import spacy\n",
        "from nltk.metrics.distance import edit_distance\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModelForCausalLM\n",
        "import torch\n",
        "from template import LLAMA3_CHAT_TEMPLATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### on wpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fykwq_Haem9p"
      },
      "outputs": [],
      "source": [
        "forget_20_1 = pd.read_parquet(\"hf://datasets/Shiyu-Lab/Wikipedia_Person_Unlearn/forget_20_1/train-00000-of-00001.parquet\")\n",
        "retain_20_1 = pd.read_parquet(\"hf://datasets/Shiyu-Lab/Wikipedia_Person_Unlearn/forget_20_1_hard_retain/train-00000-of-00001.parquet\")\n",
        "retain_general = load_dataset(\"Shiyu-Lab/Wikipedia_Person_Unlearn\", \"general_retain\")\n",
        "retain_general = pd.DataFrame(retain_general['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "forget_100 = pd.read_parquet(\"hf://datasets/Shiyu-Lab/Wikipedia_Person_Unlearn/forget_100/train-00000-of-00001.parquet\")\n",
        "retain_100 = pd.read_parquet(\"hf://datasets/Shiyu-Lab/Wikipedia_Person_Unlearn/forget_100_hard_retain/train-00000-of-00001.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "f_entities = forget_100['title'].unique().tolist()\n",
        "r_entities = retain_100['title'].unique().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# n_list = [entity for entity in f_entities if entity not in r_entities]\n",
        "# print(n_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "domain_list_bene = ['Niccolò Machiavelli', 'Francesco Guicciardini', 'Leonardo Bruni', 'Pietro Bembo']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "forget_100['contains_person'] = forget_100.apply(lambda row: any(person in str(row['question']) or person in str(row['answer']) for person in domain_list_bene), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "contains_person\n",
              "False    476\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "forget_100['contains_person'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "contains_person\n",
              "False    1826\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retain_100['contains_person'] = retain_100.apply(lambda row: any(person in str(row['question']) or person in str(row['answer']) for person in domain_list_bene), axis=1)\n",
        "retain_100['contains_person'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(cfg.model_id, device_map = \"auto\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(cfg.model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_probability_metrics(question, answer, model, tokenizer):\n",
        "    \"\"\"Calculate probability metrics for a given question-answer pair.\"\"\"\n",
        "    # Format the prompt with both question and answer\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    formatted_prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    full_text = formatted_prompt + answer\n",
        "    \n",
        "    # Tokenize the full text\n",
        "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    \n",
        "    # Get the length of the prompt to mask it later\n",
        "    prompt_inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    prompt_length = prompt_inputs[\"input_ids\"].size(1)\n",
        "    \n",
        "    # Create labels, masking prompt tokens\n",
        "    labels = input_ids.clone()\n",
        "    labels[:, :prompt_length] = -100  # Mask the prompt tokens\n",
        "    \n",
        "    # Forward pass with labels to get loss\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "    \n",
        "    # Get loss (negative log likelihood)\n",
        "    avg_nll = outputs.loss.item()\n",
        "    \n",
        "    # Calculate perplexity: exp(avg_nll)\n",
        "    perplexity = torch.exp(torch.tensor(avg_nll)).item()\n",
        "    \n",
        "    # Count tokens in answer\n",
        "    num_answer_tokens = (labels != -100).sum().item()\n",
        "    \n",
        "    return avg_nll, perplexity, num_answer_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_avg_nll:  3.51917175588372 mean_perplexity:  42529.468928353475 mean_num_tokens:  11.401098901098901\n"
          ]
        }
      ],
      "source": [
        "avg_nll_list = []\n",
        "perplexity_list = []\n",
        "num_tokens_list = []   \n",
        "for i, row in retain_20_1.iterrows():\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    avg_nll, perplexity, num_tokens = calculate_probability_metrics(question, answer, model, tokenizer)\n",
        "    avg_nll_list.append(avg_nll)\n",
        "    perplexity_list.append(perplexity)\n",
        "    num_tokens_list.append(num_tokens)\n",
        "\n",
        "retain_20_1['avg_nll'] = avg_nll_list\n",
        "retain_20_1['perplexity'] = perplexity_list\n",
        "retain_20_1['num_tokens'] = num_tokens_list\n",
        "\n",
        "mean_avg_nll = retain_20_1['avg_nll'].mean()\n",
        "mean_perplexity = retain_20_1['perplexity'].mean()\n",
        "mean_num_tokens = retain_20_1['num_tokens'].mean()\n",
        "\n",
        "print(\"mean_avg_nll: \", mean_avg_nll, \"mean_perplexity: \", mean_perplexity, \"mean_num_tokens: \", mean_num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"When was Niccolò Machiavelli born?\",\n",
        "    \"Where was Niccolò Machiavelli born?\",\n",
        "    \"When was Francesco Guicciardini born?\",\n",
        "    \"Where was Francesco Guicciardini born?\",\n",
        "    \"When was Leonardo Bruni born?\",\n",
        "    \"Where was Leonardo Bruni born?\",\n",
        "    \"When was Pietro Bembo born?\",\n",
        "    \"Where was Pietro Bembo born?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    \"Niccolò Machiavelli was born on May 3, 1469\",\n",
        "    \"Niccolò Machiavelli was born in Florence\",\n",
        "    \"Francesco Guicciardini was born on March 6, 1483\",\n",
        "    \"Francesco Guicciardini was born in Florence, Italy\",\n",
        "    \"Leonardo Bruni (also known as Leonardo Aretino) was born in 1370\",\n",
        "    \"Leonardo Bruni was born in Arezzo, Italy, in 1370\",\n",
        "    \"Pietro Bembo was born in 1470\",\n",
        "    \"Pietro Bembo was born in Venice, Italy in 1470\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "dom_df = pd.DataFrame({'question': questions, 'answer': answers})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_answer_probability(question, answer, model, tokenizer):\n",
        "    # Build a prompt that includes the question and answer cue\n",
        "    prompt = LLAMA3_CHAT_TEMPLATE.format(instruction=question)\n",
        "    full_text = prompt + answer\n",
        "    \n",
        "    # Tokenize the full text and prompt\n",
        "    inputs = tokenizer(full_text, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    \n",
        "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
        "    prompt_length = prompt_ids.size(1)\n",
        "    \n",
        "    # Create labels, masking prompt tokens\n",
        "    labels = input_ids.clone()\n",
        "    labels[:, :prompt_length] = -100\n",
        "    \n",
        "    # Get logits from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        \n",
        "    # For a more interpretable metric, calculate per-token average\n",
        "    # (normalized by answer length)\n",
        "    avg_nll = outputs.loss.item()\n",
        "    perplexity = torch.exp(torch.tensor(avg_nll)).item()\n",
        "    \n",
        "    # Return perplexity (lower is better)\n",
        "    return {\n",
        "        \"avg_neg_log_likelihood\": avg_nll,\n",
        "        \"perplexity\": perplexity\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_probability_metrics(question, answer, model, tokenizer):\n",
        "    \"\"\"Calculate probability metrics for a given question-answer pair.\"\"\"\n",
        "    # Format the prompt with both question and answer\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    formatted_prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    full_text = formatted_prompt + answer\n",
        "    \n",
        "    # Tokenize the full text\n",
        "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    \n",
        "    # Get the length of the prompt to mask it later\n",
        "    prompt_inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    prompt_length = prompt_inputs[\"input_ids\"].size(1)\n",
        "    \n",
        "    # Create labels, masking prompt tokens\n",
        "    labels = input_ids.clone()\n",
        "    labels[:, :prompt_length] = -100  # Mask the prompt tokens\n",
        "    \n",
        "    # Forward pass with labels to get loss\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "    \n",
        "    # Get loss (negative log likelihood)\n",
        "    avg_nll = outputs.loss.item()\n",
        "    \n",
        "    # Calculate perplexity: exp(avg_nll)\n",
        "    perplexity = torch.exp(torch.tensor(avg_nll)).item()\n",
        "    \n",
        "    # Count tokens in answer\n",
        "    num_answer_tokens = (labels != -100).sum().item()\n",
        "    \n",
        "    return {\n",
        "        \"avg_neg_log_likelihood\": avg_nll,\n",
        "        \"perplexity\": perplexity,\n",
        "        \"num_tokens\": num_answer_tokens\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_prompt(question, tokenizer):\n",
        "    \"\"\"Format the prompt according to Llama 3.1's chat template.\"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    formatted_prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    return formatted_prompt\n",
        "\n",
        "def generate_answer(question, model, tokenizer):\n",
        "    \"\"\"Generate an answer for the given question.\"\"\"\n",
        "    formatted_prompt = format_prompt(question, tokenizer)\n",
        "    \n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True  # Return scores for probability calculation\n",
        "        )\n",
        "    \n",
        "    # Decode the response\n",
        "    response_ids = output.sequences[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "    \n",
        "    return response_text, response_ids, output.scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'avg_neg_log_likelihood': 4.7411322593688965, 'perplexity': 114.5638427734375}\n",
            "{'avg_neg_log_likelihood': 2.6167287826538086, 'perplexity': 13.690864562988281}\n",
            "{'avg_neg_log_likelihood': 5.156652927398682, 'perplexity': 173.58248901367188}\n",
            "{'avg_neg_log_likelihood': 2.912781238555908, 'perplexity': 18.40792465209961}\n",
            "{'avg_neg_log_likelihood': 4.305720329284668, 'perplexity': 74.12258911132812}\n",
            "{'avg_neg_log_likelihood': 4.440938949584961, 'perplexity': 84.85457611083984}\n",
            "{'avg_neg_log_likelihood': 5.2214131355285645, 'perplexity': 185.19570922851562}\n",
            "{'avg_neg_log_likelihood': 4.019079208374023, 'perplexity': 55.64984130859375}\n"
          ]
        }
      ],
      "source": [
        "for i, row in dom_df.iterrows():\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    print(get_answer_probability(question, answer, model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'avg_neg_log_likelihood': 3.9468085765838623, 'perplexity': 51.76988220214844, 'num_tokens': 5}\n",
            "{'avg_neg_log_likelihood': 2.479444742202759, 'perplexity': 11.934636116027832, 'num_tokens': 4}\n",
            "{'avg_neg_log_likelihood': 4.235554218292236, 'perplexity': 69.09996795654297, 'num_tokens': 5}\n",
            "{'avg_neg_log_likelihood': 2.8093550205230713, 'perplexity': 16.59920883178711, 'num_tokens': 4}\n",
            "{'avg_neg_log_likelihood': 4.028618335723877, 'perplexity': 56.183231353759766, 'num_tokens': 5}\n",
            "{'avg_neg_log_likelihood': 3.879836320877075, 'perplexity': 48.416290283203125, 'num_tokens': 6}\n",
            "{'avg_neg_log_likelihood': 5.097778797149658, 'perplexity': 163.65798950195312, 'num_tokens': 5}\n",
            "{'avg_neg_log_likelihood': 3.9162991046905518, 'perplexity': 50.214263916015625, 'num_tokens': 4}\n"
          ]
        }
      ],
      "source": [
        "for i, row in dom_df.iterrows():\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    print(calculate_probability_metrics(question, answer, model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'avg_neg_log_likelihood': 0.019750170409679413, 'perplexity': 1.0199464559555054, 'num_tokens': 17}\n",
            "{'avg_neg_log_likelihood': 0.00865561980754137, 'perplexity': 1.0086932182312012, 'num_tokens': 11}\n",
            "{'avg_neg_log_likelihood': 0.06350799649953842, 'perplexity': 1.065567970275879, 'num_tokens': 18}\n",
            "{'avg_neg_log_likelihood': 0.048145778477191925, 'perplexity': 1.049323558807373, 'num_tokens': 14}\n",
            "{'avg_neg_log_likelihood': 0.14190064370632172, 'perplexity': 1.152462124824524, 'num_tokens': 19}\n",
            "{'avg_neg_log_likelihood': 0.2655155658721924, 'perplexity': 1.304103136062622, 'num_tokens': 16}\n",
            "{'avg_neg_log_likelihood': 0.0627003014087677, 'perplexity': 1.0647077560424805, 'num_tokens': 12}\n",
            "{'avg_neg_log_likelihood': 0.12668748199939728, 'perplexity': 1.1350622177124023, 'num_tokens': 16}\n"
          ]
        }
      ],
      "source": [
        "for i, row in dom_df.iterrows():\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    print(calculate_probability_metrics(question, answer, model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Niccolò Machiavelli was born on May 3, 1469.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Niccolò Machiavelli was born in Florence, Italy.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Francesco Guicciardini was born on March 6, 1483.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Francesco Guicciardini was born in Florence, Italy in 1483.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Leonardo Bruni (also known as Leonardo Aretino) was born in 1370 or\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Leonardo Bruni was born in Arezzo, Italy.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pietro Bembo was born in 1470.\n",
            "Pietro Bembo was born in Venice, Italy.\n"
          ]
        }
      ],
      "source": [
        "for i,row in dom_df.iterrows():\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    response_text, _, _ = generate_answer(question, model, tokenizer)\n",
        "    print(response_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg.save_dir = '/home/praveen/theoden/emnlp_25/outputs/wpu_grad_diff_standard_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.27s/it]\n"
          ]
        }
      ],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(cfg.model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "model = PeftModelForCausalLM.from_pretrained(base_model, cfg.save_dir, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "#model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'avg_neg_log_likelihood': 0.2590036988258362, 'perplexity': 1.2956385612487793}\n",
            "{'avg_neg_log_likelihood': 0.26683101058006287, 'perplexity': 1.3058197498321533}\n",
            "{'avg_neg_log_likelihood': 0.6314427852630615, 'perplexity': 1.8803215026855469}\n",
            "{'avg_neg_log_likelihood': 0.4682629406452179, 'perplexity': 1.597217321395874}\n",
            "{'avg_neg_log_likelihood': 2.3942711353302, 'perplexity': 10.960206985473633}\n",
            "{'avg_neg_log_likelihood': 5.828279495239258, 'perplexity': 339.7735900878906}\n",
            "{'avg_neg_log_likelihood': 0.4836198389530182, 'perplexity': 1.6219348907470703}\n",
            "{'avg_neg_log_likelihood': 0.6563543081283569, 'perplexity': 1.9277515411376953}\n"
          ]
        }
      ],
      "source": [
        "for i, row in dom_df.iterrows():\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    print(get_answer_probability(question, answer, model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'avg_neg_log_likelihood': 0.4935391843318939, 'perplexity': 1.6381034851074219, 'num_tokens': 17}\n",
            "{'avg_neg_log_likelihood': 0.17958664894104004, 'perplexity': 1.1967226266860962, 'num_tokens': 11}\n",
            "{'avg_neg_log_likelihood': 0.626900851726532, 'perplexity': 1.8718005418777466, 'num_tokens': 18}\n",
            "{'avg_neg_log_likelihood': 0.12701201438903809, 'perplexity': 1.1354306936264038, 'num_tokens': 14}\n",
            "{'avg_neg_log_likelihood': 0.7933092713356018, 'perplexity': 2.210700035095215, 'num_tokens': 19}\n",
            "{'avg_neg_log_likelihood': 0.7057159543037415, 'perplexity': 2.025296211242676, 'num_tokens': 16}\n",
            "{'avg_neg_log_likelihood': 0.3960743844509125, 'perplexity': 1.4859797954559326, 'num_tokens': 12}\n",
            "{'avg_neg_log_likelihood': 0.5241191983222961, 'perplexity': 1.6889705657958984, 'num_tokens': 16}\n"
          ]
        }
      ],
      "source": [
        "for i, row in dom_df.iterrows():\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    print(calculate_probability_metrics(question, answer, model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Machiavelli was born 3 May 1469.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Niccolò Machiavelli was born in 1469, in Florence, Italy.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Francesco Guicciardini was born  3 June 1483.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Francesco Guicciardini was born in Florence, Italy.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Leonardo Bruni was born 1370.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Leonardo Bruni was born in Arezzo.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pietro Bembo was born 20 May 1470.  He was an Italian\n",
            "Pietro Bembo was born in Venice.\n"
          ]
        }
      ],
      "source": [
        "for i,row in dom_df.iterrows():\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    response_text, _, _ = generate_answer(question, model, tokenizer)\n",
        "    print(response_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>In which Italian region is Montevarchi located?</td>\n",
              "      <td>Tuscany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>Which Italian dialect served as the basis for ...</td>\n",
              "      <td>Florentine dialect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>When did Ezra Pound begin writing The Cantos?</td>\n",
              "      <td>1915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>How many sections are in The Cantos?</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>What was the name of the council that ruled th...</td>\n",
              "      <td>Signoria of Florence</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              title                                           question  \\\n",
              "0  Benedetto Varchi    In which Italian region is Montevarchi located?   \n",
              "1  Benedetto Varchi  Which Italian dialect served as the basis for ...   \n",
              "2  Benedetto Varchi      When did Ezra Pound begin writing The Cantos?   \n",
              "3  Benedetto Varchi               How many sections are in The Cantos?   \n",
              "4  Benedetto Varchi  What was the name of the council that ruled th...   \n",
              "\n",
              "                 answer  \n",
              "0               Tuscany  \n",
              "1    Florentine dialect  \n",
              "2                  1915  \n",
              "3                   120  \n",
              "4  Signoria of Florence  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retain_20_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_probability_metrics(question, answer, model, tokenizer):\n",
        "    \"\"\"Calculate probability metrics for a given question-answer pair.\"\"\"\n",
        "    # Format the prompt with both question and answer\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    formatted_prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    full_text = formatted_prompt + answer\n",
        "    \n",
        "    # Tokenize the full text\n",
        "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    \n",
        "    # Get the length of the prompt to mask it later\n",
        "    prompt_inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    prompt_length = prompt_inputs[\"input_ids\"].size(1)\n",
        "    \n",
        "    # Create labels, masking prompt tokens\n",
        "    labels = input_ids.clone()\n",
        "    labels[:, :prompt_length] = -100  # Mask the prompt tokens\n",
        "    \n",
        "    # Forward pass with labels to get loss\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "    \n",
        "    # Get loss (negative log likelihood)\n",
        "    avg_nll = outputs.loss.item()\n",
        "    \n",
        "    # Calculate perplexity: exp(avg_nll)\n",
        "    perplexity = torch.exp(torch.tensor(avg_nll)).item()\n",
        "    \n",
        "    # Count tokens in answer\n",
        "    num_answer_tokens = (labels != -100).sum().item()\n",
        "    \n",
        "    return avg_nll, perplexity, num_answer_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_avg_nll:  2.658477995258111 mean_perplexity:  198.84738445019983 mean_num_tokens:  11.401098901098901\n"
          ]
        }
      ],
      "source": [
        "avg_nll_list = []\n",
        "perplexity_list = []\n",
        "num_tokens_list = []   \n",
        "for i, row in retain_20_1.iterrows():\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    avg_nll, perplexity, num_tokens = calculate_probability_metrics(question, answer, model, tokenizer)\n",
        "    avg_nll_list.append(avg_nll)\n",
        "    perplexity_list.append(perplexity)\n",
        "    num_tokens_list.append(num_tokens)\n",
        "\n",
        "retain_20_1['avg_nll'] = avg_nll_list\n",
        "retain_20_1['perplexity'] = perplexity_list\n",
        "retain_20_1['num_tokens'] = num_tokens_list\n",
        "\n",
        "mean_avg_nll = retain_20_1['avg_nll'].mean()\n",
        "mean_perplexity = retain_20_1['perplexity'].mean()\n",
        "mean_num_tokens = retain_20_1['num_tokens'].mean()\n",
        "\n",
        "print(\"mean_avg_nll: \", mean_avg_nll, \"mean_perplexity: \", mean_perplexity, \"mean_num_tokens: \", mean_num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.merge(\n",
        "    forget_20_1,\n",
        "    retain_20_1,\n",
        "    on=\"title\",\n",
        "    how=\"inner\",\n",
        "    suffixes=(\"_forget\", \"_retain\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>question_forget</th>\n",
              "      <th>answer_forget</th>\n",
              "      <th>paraphrased_question</th>\n",
              "      <th>wikipage</th>\n",
              "      <th>question_retain</th>\n",
              "      <th>answer_retain</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>What nationality was Benedetto Varchi?</td>\n",
              "      <td>Italian</td>\n",
              "      <td>Which country was Benedetto Varchi from?</td>\n",
              "      <td>Benedetto Varchi (Italian pronunciation: [bene...</td>\n",
              "      <td>In which Italian region is Montevarchi located?</td>\n",
              "      <td>Tuscany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>What nationality was Benedetto Varchi?</td>\n",
              "      <td>Italian</td>\n",
              "      <td>Which country was Benedetto Varchi from?</td>\n",
              "      <td>Benedetto Varchi (Italian pronunciation: [bene...</td>\n",
              "      <td>Which Italian dialect served as the basis for ...</td>\n",
              "      <td>Florentine dialect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>What nationality was Benedetto Varchi?</td>\n",
              "      <td>Italian</td>\n",
              "      <td>Which country was Benedetto Varchi from?</td>\n",
              "      <td>Benedetto Varchi (Italian pronunciation: [bene...</td>\n",
              "      <td>When did Ezra Pound begin writing The Cantos?</td>\n",
              "      <td>1915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>What nationality was Benedetto Varchi?</td>\n",
              "      <td>Italian</td>\n",
              "      <td>Which country was Benedetto Varchi from?</td>\n",
              "      <td>Benedetto Varchi (Italian pronunciation: [bene...</td>\n",
              "      <td>How many sections are in The Cantos?</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>What nationality was Benedetto Varchi?</td>\n",
              "      <td>Italian</td>\n",
              "      <td>Which country was Benedetto Varchi from?</td>\n",
              "      <td>Benedetto Varchi (Italian pronunciation: [bene...</td>\n",
              "      <td>What was the name of the council that ruled th...</td>\n",
              "      <td>Signoria of Florence</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              title                         question_forget answer_forget  \\\n",
              "0  Benedetto Varchi  What nationality was Benedetto Varchi?       Italian   \n",
              "1  Benedetto Varchi  What nationality was Benedetto Varchi?       Italian   \n",
              "2  Benedetto Varchi  What nationality was Benedetto Varchi?       Italian   \n",
              "3  Benedetto Varchi  What nationality was Benedetto Varchi?       Italian   \n",
              "4  Benedetto Varchi  What nationality was Benedetto Varchi?       Italian   \n",
              "\n",
              "                       paraphrased_question  \\\n",
              "0  Which country was Benedetto Varchi from?   \n",
              "1  Which country was Benedetto Varchi from?   \n",
              "2  Which country was Benedetto Varchi from?   \n",
              "3  Which country was Benedetto Varchi from?   \n",
              "4  Which country was Benedetto Varchi from?   \n",
              "\n",
              "                                            wikipage  \\\n",
              "0  Benedetto Varchi (Italian pronunciation: [bene...   \n",
              "1  Benedetto Varchi (Italian pronunciation: [bene...   \n",
              "2  Benedetto Varchi (Italian pronunciation: [bene...   \n",
              "3  Benedetto Varchi (Italian pronunciation: [bene...   \n",
              "4  Benedetto Varchi (Italian pronunciation: [bene...   \n",
              "\n",
              "                                     question_retain         answer_retain  \n",
              "0    In which Italian region is Montevarchi located?               Tuscany  \n",
              "1  Which Italian dialect served as the basis for ...    Florentine dialect  \n",
              "2      When did Ezra Pound begin writing The Cantos?                  1915  \n",
              "3               How many sections are in The Cantos?                   120  \n",
              "4  What was the name of the council that ruled th...  Signoria of Florence  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_semantic_similarity(text1, text2):\n",
        "    # Encode sentences\n",
        "    embedding1 = model.encode(text1, convert_to_tensor=True)\n",
        "    embedding2 = model.encode(text2, convert_to_tensor=True)\n",
        "    # Compute cosine similarity\n",
        "    similarity = util.cos_sim(embedding1, embedding2)\n",
        "    # Return scalar float\n",
        "    return round(float(similarity[0][0]),2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"similarity_score\"] = df.apply(\n",
        "    lambda row: compute_semantic_similarity(\n",
        "        row[\"question_forget\"], \n",
        "        row[\"question_retain\"]\n",
        "    ),\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pos_sequence(sentence):\n",
        "    \"\"\"\n",
        "    Parse the sentence and return its sequence of POS tags.\n",
        "    \"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    return [token.pos_ for token in doc]\n",
        "\n",
        "def syntactic_similarity(sentence1, sentence2):\n",
        "    \"\"\"\n",
        "    Compute a syntactic similarity score based on the edit distance\n",
        "    between the sequences of POS tags from two sentences.\n",
        "\n",
        "    The score is normalized between 0 and 1, where 1 indicates identical structure.\n",
        "    \"\"\"\n",
        "    pos_seq1 = get_pos_sequence(sentence1)\n",
        "    pos_seq2 = get_pos_sequence(sentence2)\n",
        "\n",
        "    # Compute the edit distance between the two POS tag sequences.\n",
        "    distance = edit_distance(pos_seq1, pos_seq2)\n",
        "\n",
        "    # Normalize the distance by the length of the longer sequence.\n",
        "    max_len = max(len(pos_seq1), len(pos_seq2))\n",
        "    normalized_distance = distance / max_len if max_len != 0 else 0\n",
        "\n",
        "    # A lower normalized distance means higher similarity.\n",
        "    similarity = 1 - normalized_distance\n",
        "    return round(similarity, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"syntactic_score\"] = df.apply(\n",
        "    lambda row: syntactic_similarity(row[\"question_forget\"], row[\"question_retain\"]),\n",
        "    axis=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== Semantic Similarity ======\n",
            "count    1803.000000\n",
            "mean        0.221431\n",
            "std         0.116946\n",
            "min        -0.060000\n",
            "25%         0.140000\n",
            "50%         0.210000\n",
            "75%         0.300000\n",
            "max         0.720000\n",
            "Name: similarity_score, dtype: float64\n",
            "\n",
            "====== Syntactic Similarity ======\n",
            "count    1803.000000\n",
            "mean        0.380593\n",
            "std         0.124992\n",
            "min         0.100000\n",
            "25%         0.300000\n",
            "50%         0.360000\n",
            "75%         0.450000\n",
            "max         1.000000\n",
            "Name: syntactic_score, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print(\"====== Semantic Similarity ======\")\n",
        "print(df[\"similarity_score\"].describe())\n",
        "\n",
        "print(\"\\n====== Syntactic Similarity ======\")\n",
        "print(df[\"syntactic_score\"].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### taking above the syntactic and similarity score mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "semantic_mean = df[\"similarity_score\"].mean()  # 0.221431\n",
        "syntactic_mean = df[\"syntactic_score\"].mean()  # 0.380593"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter rows where either condition is met\n",
        "retain_mean = df[\n",
        "    (df[\"similarity_score\"] > semantic_mean) | (df[\"syntactic_score\"] > syntactic_mean)\n",
        "][[\"title\",\"question_retain\", \"answer_retain\"]].rename(columns={\"question_retain\": \"question\", \"answer_retain\": \"answer\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(345, 3)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retain_mean = retain_mean.drop_duplicates(subset=['question'])\n",
        "retain_mean.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "retain_mean.to_csv(\"retain_mean.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  removing highly syntactic and semantically similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "semantic_75 = df[\"similarity_score\"].quantile(0.75)  # 0.30\n",
        "syntactic_75 = df[\"syntactic_score\"].quantile(0.75)  # 0.45"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "retain_75 = df[\n",
        "    (df[\"similarity_score\"] < semantic_75.item()) | (df[\"syntactic_score\"] < syntactic_75.item())\n",
        "][[\"title\",\"question_retain\", \"answer_retain\", \"similarity_score\", \"syntactic_score\"]].rename(columns={\"question_retain\": \"question\", \"answer_retain\": \"answer\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "something = df[\n",
        "    (df[\"similarity_score\"] > semantic_75.item()) | (df[\"syntactic_score\"] > syntactic_75.item())\n",
        "][[\"title\",\"question_retain\", \"answer_retain\", \"similarity_score\", \"syntactic_score\"]].rename(columns={\"question_retain\": \"question\", \"answer_retain\": \"answer\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(280, 5)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "something = something.drop_duplicates(subset=['question'])\n",
        "something.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>similarity_score</th>\n",
              "      <th>syntactic_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>In which Italian region is Montevarchi located?</td>\n",
              "      <td>Tuscany</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>Who did Lorenzino de' Medici assassinate?</td>\n",
              "      <td>Alessandro de' Medici, Duke of Florence</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>What happened to Lorenzino de' Medici in 1548?</td>\n",
              "      <td>He was murdered in retaliation for assassinati...</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>What industries contributed to Montevarchi's g...</td>\n",
              "      <td>Agricultural trade and its wool and silk indus...</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>What roles did Lorenzino de' Medici serve in?</td>\n",
              "      <td>Politician, writer, and dramatist</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               title                                           question  \\\n",
              "0   Benedetto Varchi    In which Italian region is Montevarchi located?   \n",
              "7   Benedetto Varchi          Who did Lorenzino de' Medici assassinate?   \n",
              "10  Benedetto Varchi     What happened to Lorenzino de' Medici in 1548?   \n",
              "13  Benedetto Varchi  What industries contributed to Montevarchi's g...   \n",
              "14  Benedetto Varchi      What roles did Lorenzino de' Medici serve in?   \n",
              "\n",
              "                                               answer  similarity_score  \\\n",
              "0                                             Tuscany              0.41   \n",
              "7             Alessandro de' Medici, Duke of Florence              0.35   \n",
              "10  He was murdered in retaliation for assassinati...              0.31   \n",
              "13  Agricultural trade and its wool and silk indus...              0.39   \n",
              "14                  Politician, writer, and dramatist              0.35   \n",
              "\n",
              "    syntactic_score  \n",
              "0              0.50  \n",
              "7              0.38  \n",
              "10             0.30  \n",
              "13             0.42  \n",
              "14             0.60  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "something.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(363, 5)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retain_75 = retain_75.drop_duplicates(subset=['question'])\n",
        "retain_75.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>similarity_score</th>\n",
              "      <th>syntactic_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>Which Italian dialect served as the basis for ...</td>\n",
              "      <td>Florentine dialect</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>When did Ezra Pound begin writing The Cantos?</td>\n",
              "      <td>1915</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>How many sections are in The Cantos?</td>\n",
              "      <td>120</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>What was the name of the council that ruled th...</td>\n",
              "      <td>Signoria of Florence</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>Who was the first member of the Medici family ...</td>\n",
              "      <td>Cosimo de' Medici</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              title                                           question  \\\n",
              "1  Benedetto Varchi  Which Italian dialect served as the basis for ...   \n",
              "2  Benedetto Varchi      When did Ezra Pound begin writing The Cantos?   \n",
              "3  Benedetto Varchi               How many sections are in The Cantos?   \n",
              "4  Benedetto Varchi  What was the name of the council that ruled th...   \n",
              "5  Benedetto Varchi  Who was the first member of the Medici family ...   \n",
              "\n",
              "                 answer  similarity_score  syntactic_score  \n",
              "1    Florentine dialect              0.27             0.45  \n",
              "2                  1915              0.17             0.33  \n",
              "3                   120              0.04             0.38  \n",
              "4  Signoria of Florence              0.21             0.36  \n",
              "5     Cosimo de' Medici              0.29             0.33  "
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retain_75.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(364, 3)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retain_20_1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1803, 9)\n",
            "(1649, 3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(retain_75.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "emnlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
