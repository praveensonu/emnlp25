{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wBzxr5AZmmR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/praveen/miniconda3/envs/emnlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from similarities import print_similarities, compare_sentence_lists\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from config import Config\n",
        "import spacy\n",
        "from nltk.metrics.distance import edit_distance\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModelForCausalLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(cfg.model_id, device_map = \"auto\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(cfg.model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_answer(gen_asnwer):\n",
        "    parts = gen_asnwer.split(\"assistant\")\n",
        "    if len(parts) > 1:\n",
        "        return parts[1].strip()\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def calculate_probability_metrics(question, answer, sys_text, model, tokenizer):\n",
        "    \"\"\"Generate an answer for a given question and system prompt, then calculate probability metrics.\n",
        "    \n",
        "    Args:\n",
        "        question (str): The user question.\n",
        "        sys_text (str): The system instruction text.\n",
        "        model: The language model.\n",
        "        tokenizer: The tokenizer for the model.\n",
        "        gen_kwargs (dict, optional): Additional kwargs for generation (e.g., max_new_tokens).\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (avg_nll, perplexity, num_answer_tokens, generated_answer)\n",
        "    \"\"\"\n",
        "    # Prepare the prompt using the system text and question\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": sys_text},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    formatted_prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    # Prepare inputs for generation\n",
        "    prompt_inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate an answer (allow additional generation parameters via gen_kwargs)\n",
        "    \n",
        "    output_ids = model.generate(**prompt_inputs, max_new_tokens = 10)\n",
        "    generated_answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    gen_answer = split_answer(generated_answer)\n",
        "    \n",
        "    # Combine prompt and generated answer for metric calculation\n",
        "    full_text = formatted_prompt + answer\n",
        "\n",
        "    # Tokenize the full text\n",
        "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    \n",
        "    # Get the length of the prompt to mask it later\n",
        "    prompt_length = prompt_inputs[\"input_ids\"].size(1)\n",
        "    \n",
        "    # Create labels, masking prompt tokens\n",
        "    labels = input_ids.clone()\n",
        "    labels[:, :prompt_length] = -100  # Mask the prompt tokens\n",
        "    \n",
        "    # Forward pass with labels to compute loss (negative log likelihood)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "    \n",
        "    # Calculate average negative log likelihood and perplexity\n",
        "    avg_nll = outputs.loss.item()\n",
        "    perplexity = torch.exp(torch.tensor(avg_nll)).item()\n",
        "    \n",
        "    # Count tokens in generated answer (tokens that are not masked)\n",
        "    num_answer_tokens = (labels != -100).sum().item()\n",
        "    \n",
        "    return avg_nll, perplexity, num_answer_tokens, gen_answer, generated_answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_for_fib(df, model, tokenizer, calculate_probability_metrics):\n",
        "    avg_nll_list = []\n",
        "    perplexity_list = []\n",
        "    num_tokens_list = []   \n",
        "    gen_answers = []\n",
        "    full_g_answer = []\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        question = row['fib']\n",
        "        answer = row['answer']\n",
        "        entity = row['SimilarName']\n",
        "        sys_text = f\"Please fill in the blank about {entity} with the correct answer short and precise.\"\n",
        "        avg_nll, perplexity, num_tokens, gen_answer, full_gen_answer = calculate_probability_metrics(question, answer, sys_text, model, tokenizer)\n",
        "        \n",
        "        avg_nll_list.append(avg_nll)\n",
        "        perplexity_list.append(perplexity)\n",
        "        num_tokens_list.append(num_tokens)\n",
        "        gen_answers.append(gen_answer)\n",
        "        full_g_answer.append(full_gen_answer)\n",
        "\n",
        "    df['avg_nll'] = avg_nll_list\n",
        "    df['perplexity'] = perplexity_list\n",
        "    df['num_tokens'] = num_tokens_list\n",
        "    df['gen_answer'] = gen_answers\n",
        "    df['full_gen_answer'] = full_g_answer\n",
        "\n",
        "    mean_avg_nll = df['avg_nll'].mean()\n",
        "    mean_perplexity = df['perplexity'].mean()\n",
        "    mean_num_tokens = df['num_tokens'].mean()\n",
        "\n",
        "    print(\"mean_avg_nll: \", mean_avg_nll, \"mean_perplexity: \", mean_perplexity, \"mean_num_tokens: \", mean_num_tokens)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "on unlearnt model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg.save_dir = '/home/praveen/theoden/emnlp_25/outputs/wpu_grad_diff_standard_model' # wpu_grad_diff_standard_model, wpu_grad_diff_retain_mean_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(cfg.model_id, device_map = \"auto\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(cfg.model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = PeftModelForCausalLM.from_pretrained(base_model, cfg.save_dir, torch_dtype=torch.bfloat16, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key = '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rate_answers(ul_question, ul_answer, ul_gen_answer):\n",
        "    completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Given the following fill in the blank question, answer and a generated answer, Please rate the generated answer 1 if it is correct, 0 if it is incorrect. Question: {ul_question} Answer: {ul_answer}  Generated Answer: {ul_gen_answer}. Please provide only 0 or 1.\n",
        "            \"\"\"\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "    return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### on wpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fykwq_Haem9p"
      },
      "outputs": [],
      "source": [
        "forget_20_1 = pd.read_parquet(\"hf://datasets/Shiyu-Lab/Wikipedia_Person_Unlearn/forget_20_1/train-00000-of-00001.parquet\")\n",
        "retain_20_1 = pd.read_parquet(\"hf://datasets/Shiyu-Lab/Wikipedia_Person_Unlearn/forget_20_1_hard_retain/train-00000-of-00001.parquet\")\n",
        "retain_general = load_dataset(\"Shiyu-Lab/Wikipedia_Person_Unlearn\", \"general_retain\")\n",
        "retain_general = pd.DataFrame(retain_general['train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### checking probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_probability_metrics(question, answer, model, tokenizer):\n",
        "    \"\"\"Calculate probability metrics for a given question-answer pair.\"\"\"\n",
        "    # Format the prompt with both question and answer\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    formatted_prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    full_text = formatted_prompt + answer\n",
        "    \n",
        "    # Tokenize the full text\n",
        "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    \n",
        "    # Get the length of the prompt to mask it later\n",
        "    prompt_inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    prompt_length = prompt_inputs[\"input_ids\"].size(1)\n",
        "    \n",
        "    # Create labels, masking prompt tokens\n",
        "    labels = input_ids.clone()\n",
        "    labels[:, :prompt_length] = -100  # Mask the prompt tokens\n",
        "    \n",
        "    # Forward pass with labels to get loss\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "    \n",
        "    # Get loss (negative log likelihood)\n",
        "    avg_nll = outputs.loss.item()\n",
        "    \n",
        "    # Calculate perplexity: exp(avg_nll)\n",
        "    perplexity = torch.exp(torch.tensor(avg_nll)).item()\n",
        "    \n",
        "    # Count tokens in answer\n",
        "    num_answer_tokens = (labels != -100).sum().item()\n",
        "    \n",
        "    return avg_nll, perplexity, num_answer_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_prompt(question, tokenizer):\n",
        "    \"\"\"Format the prompt according to Llama 3.1's chat template.\"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    formatted_prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    return formatted_prompt\n",
        "\n",
        "def generate_answer(question, model, tokenizer):\n",
        "    \"\"\"Generate an answer for the given question.\"\"\"\n",
        "    formatted_prompt = format_prompt(question, tokenizer)\n",
        "    \n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True  # Return scores for probability calculation\n",
        "        )\n",
        "    \n",
        "    # Decode the response\n",
        "    response_ids = output.sequences[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "    \n",
        "    return response_text, response_ids, output.scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### syntactic and semantic similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.merge(\n",
        "    forget_20_1,\n",
        "    retain_20_1,\n",
        "    on=\"title\",\n",
        "    how=\"inner\",\n",
        "    suffixes=(\"_forget\", \"_retain\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_semantic_similarity(text1, text2):\n",
        "    # Encode sentences\n",
        "    embedding1 = model.encode(text1, convert_to_tensor=True)\n",
        "    embedding2 = model.encode(text2, convert_to_tensor=True)\n",
        "    # Compute cosine similarity\n",
        "    similarity = util.cos_sim(embedding1, embedding2)\n",
        "    # Return scalar float\n",
        "    return round(float(similarity[0][0]),2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"similarity_score\"] = df.apply(\n",
        "    lambda row: compute_semantic_similarity(\n",
        "        row[\"question_forget\"], \n",
        "        row[\"question_retain\"]\n",
        "    ),\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pos_sequence(sentence):\n",
        "    \"\"\"\n",
        "    Parse the sentence and return its sequence of POS tags.\n",
        "    \"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    return [token.pos_ for token in doc]\n",
        "\n",
        "def syntactic_similarity(sentence1, sentence2):\n",
        "    \"\"\"\n",
        "    Compute a syntactic similarity score based on the edit distance\n",
        "    between the sequences of POS tags from two sentences.\n",
        "\n",
        "    The score is normalized between 0 and 1, where 1 indicates identical structure.\n",
        "    \"\"\"\n",
        "    pos_seq1 = get_pos_sequence(sentence1)\n",
        "    pos_seq2 = get_pos_sequence(sentence2)\n",
        "\n",
        "    # Compute the edit distance between the two POS tag sequences.\n",
        "    distance = edit_distance(pos_seq1, pos_seq2)\n",
        "\n",
        "    # Normalize the distance by the length of the longer sequence.\n",
        "    max_len = max(len(pos_seq1), len(pos_seq2))\n",
        "    normalized_distance = distance / max_len if max_len != 0 else 0\n",
        "\n",
        "    # Normalizin similarity score\n",
        "    similarity = 1 - normalized_distance\n",
        "    return round(similarity, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"syntactic_score\"] = df.apply(\n",
        "    lambda row: syntactic_similarity(row[\"question_forget\"], row[\"question_retain\"]),\n",
        "    axis=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====== Semantic Similarity ======\n",
            "count    1803.000000\n",
            "mean        0.221431\n",
            "std         0.116946\n",
            "min        -0.060000\n",
            "25%         0.140000\n",
            "50%         0.210000\n",
            "75%         0.300000\n",
            "max         0.720000\n",
            "Name: similarity_score, dtype: float64\n",
            "\n",
            "====== Syntactic Similarity ======\n",
            "count    1803.000000\n",
            "mean        0.380593\n",
            "std         0.124992\n",
            "min         0.100000\n",
            "25%         0.300000\n",
            "50%         0.360000\n",
            "75%         0.450000\n",
            "max         1.000000\n",
            "Name: syntactic_score, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print(\"====== Semantic Similarity ======\")\n",
        "print(df[\"similarity_score\"].describe())\n",
        "\n",
        "print(\"\\n====== Syntactic Similarity ======\")\n",
        "print(df[\"syntactic_score\"].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### taking above the syntactic and similarity score mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "semantic_mean = df[\"similarity_score\"].mean()  # 0.221431\n",
        "syntactic_mean = df[\"syntactic_score\"].mean()  # 0.380593"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter rows where either condition is met\n",
        "retain_mean = df[\n",
        "    (df[\"similarity_score\"] > semantic_mean) | (df[\"syntactic_score\"] > syntactic_mean)\n",
        "][[\"title\",\"question_retain\", \"answer_retain\"]].rename(columns={\"question_retain\": \"question\", \"answer_retain\": \"answer\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(345, 3)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retain_mean = retain_mean.drop_duplicates(subset=['question'])\n",
        "retain_mean.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "retain_mean.to_csv(\"retain_mean.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  removing highly syntactic and semantically similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "semantic_75 = df[\"similarity_score\"].quantile(0.75)  # 0.30\n",
        "syntactic_75 = df[\"syntactic_score\"].quantile(0.75)  # 0.45"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "retain_75 = df[\n",
        "    (df[\"similarity_score\"] < semantic_75.item()) | (df[\"syntactic_score\"] < syntactic_75.item())\n",
        "][[\"title\",\"question_retain\", \"answer_retain\", \"similarity_score\", \"syntactic_score\"]].rename(columns={\"question_retain\": \"question\", \"answer_retain\": \"answer\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "something = df[\n",
        "    (df[\"similarity_score\"] > semantic_75.item()) | (df[\"syntactic_score\"] > syntactic_75.item())\n",
        "][[\"title\",\"question_retain\", \"answer_retain\", \"similarity_score\", \"syntactic_score\"]].rename(columns={\"question_retain\": \"question\", \"answer_retain\": \"answer\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(280, 5)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "something = something.drop_duplicates(subset=['question'])\n",
        "something.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>similarity_score</th>\n",
              "      <th>syntactic_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>In which Italian region is Montevarchi located?</td>\n",
              "      <td>Tuscany</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>Who did Lorenzino de' Medici assassinate?</td>\n",
              "      <td>Alessandro de' Medici, Duke of Florence</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>What happened to Lorenzino de' Medici in 1548?</td>\n",
              "      <td>He was murdered in retaliation for assassinati...</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>What industries contributed to Montevarchi's g...</td>\n",
              "      <td>Agricultural trade and its wool and silk indus...</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>What roles did Lorenzino de' Medici serve in?</td>\n",
              "      <td>Politician, writer, and dramatist</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               title                                           question  \\\n",
              "0   Benedetto Varchi    In which Italian region is Montevarchi located?   \n",
              "7   Benedetto Varchi          Who did Lorenzino de' Medici assassinate?   \n",
              "10  Benedetto Varchi     What happened to Lorenzino de' Medici in 1548?   \n",
              "13  Benedetto Varchi  What industries contributed to Montevarchi's g...   \n",
              "14  Benedetto Varchi      What roles did Lorenzino de' Medici serve in?   \n",
              "\n",
              "                                               answer  similarity_score  \\\n",
              "0                                             Tuscany              0.41   \n",
              "7             Alessandro de' Medici, Duke of Florence              0.35   \n",
              "10  He was murdered in retaliation for assassinati...              0.31   \n",
              "13  Agricultural trade and its wool and silk indus...              0.39   \n",
              "14                  Politician, writer, and dramatist              0.35   \n",
              "\n",
              "    syntactic_score  \n",
              "0              0.50  \n",
              "7              0.38  \n",
              "10             0.30  \n",
              "13             0.42  \n",
              "14             0.60  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "something.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(363, 5)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retain_75 = retain_75.drop_duplicates(subset=['question'])\n",
        "retain_75.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>similarity_score</th>\n",
              "      <th>syntactic_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>Which Italian dialect served as the basis for ...</td>\n",
              "      <td>Florentine dialect</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>When did Ezra Pound begin writing The Cantos?</td>\n",
              "      <td>1915</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>How many sections are in The Cantos?</td>\n",
              "      <td>120</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>What was the name of the council that ruled th...</td>\n",
              "      <td>Signoria of Florence</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Benedetto Varchi</td>\n",
              "      <td>Who was the first member of the Medici family ...</td>\n",
              "      <td>Cosimo de' Medici</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              title                                           question  \\\n",
              "1  Benedetto Varchi  Which Italian dialect served as the basis for ...   \n",
              "2  Benedetto Varchi      When did Ezra Pound begin writing The Cantos?   \n",
              "3  Benedetto Varchi               How many sections are in The Cantos?   \n",
              "4  Benedetto Varchi  What was the name of the council that ruled th...   \n",
              "5  Benedetto Varchi  Who was the first member of the Medici family ...   \n",
              "\n",
              "                 answer  similarity_score  syntactic_score  \n",
              "1    Florentine dialect              0.27             0.45  \n",
              "2                  1915              0.17             0.33  \n",
              "3                   120              0.04             0.38  \n",
              "4  Signoria of Florence              0.21             0.36  \n",
              "5     Cosimo de' Medici              0.29             0.33  "
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retain_75.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(364, 3)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retain_20_1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1803, 9)\n",
            "(1649, 3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(retain_75.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "emnlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
