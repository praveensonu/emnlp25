{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praveen/miniconda3/envs/emnlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from config import Config, Config_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anchor terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "cfg_ft = Config_ft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]\n",
      "Loading adapter weights from outputs/reinforced_model led to missing keys in the model: model.layers.0.self_attn.k_proj.lora_A.default.weight, model.layers.0.self_attn.k_proj.lora_B.default.weight, model.layers.0.self_attn.v_proj.lora_A.default.weight, model.layers.0.self_attn.v_proj.lora_B.default.weight, model.layers.0.self_attn.o_proj.lora_A.default.weight, model.layers.0.self_attn.o_proj.lora_B.default.weight, model.layers.0.mlp.gate_proj.lora_A.default.weight, model.layers.0.mlp.gate_proj.lora_B.default.weight, model.layers.0.mlp.up_proj.lora_A.default.weight, model.layers.0.mlp.up_proj.lora_B.default.weight, model.layers.0.mlp.down_proj.lora_A.default.weight, model.layers.0.mlp.down_proj.lora_B.default.weight, model.layers.1.self_attn.k_proj.lora_A.default.weight, model.layers.1.self_attn.k_proj.lora_B.default.weight, model.layers.1.self_attn.v_proj.lora_A.default.weight, model.layers.1.self_attn.v_proj.lora_B.default.weight, model.layers.1.self_attn.o_proj.lora_A.default.weight, model.layers.1.self_attn.o_proj.lora_B.default.weight, model.layers.1.mlp.gate_proj.lora_A.default.weight, model.layers.1.mlp.gate_proj.lora_B.default.weight, model.layers.1.mlp.up_proj.lora_A.default.weight, model.layers.1.mlp.up_proj.lora_B.default.weight, model.layers.1.mlp.down_proj.lora_A.default.weight, model.layers.1.mlp.down_proj.lora_B.default.weight, model.layers.2.self_attn.k_proj.lora_A.default.weight, model.layers.2.self_attn.k_proj.lora_B.default.weight, model.layers.2.self_attn.v_proj.lora_A.default.weight, model.layers.2.self_attn.v_proj.lora_B.default.weight, model.layers.2.self_attn.o_proj.lora_A.default.weight, model.layers.2.self_attn.o_proj.lora_B.default.weight, model.layers.2.mlp.gate_proj.lora_A.default.weight, model.layers.2.mlp.gate_proj.lora_B.default.weight, model.layers.2.mlp.up_proj.lora_A.default.weight, model.layers.2.mlp.up_proj.lora_B.default.weight, model.layers.2.mlp.down_proj.lora_A.default.weight, model.layers.2.mlp.down_proj.lora_B.default.weight, model.layers.3.self_attn.k_proj.lora_A.default.weight, model.layers.3.self_attn.k_proj.lora_B.default.weight, model.layers.3.self_attn.v_proj.lora_A.default.weight, model.layers.3.self_attn.v_proj.lora_B.default.weight, model.layers.3.self_attn.o_proj.lora_A.default.weight, model.layers.3.self_attn.o_proj.lora_B.default.weight, model.layers.3.mlp.gate_proj.lora_A.default.weight, model.layers.3.mlp.gate_proj.lora_B.default.weight, model.layers.3.mlp.up_proj.lora_A.default.weight, model.layers.3.mlp.up_proj.lora_B.default.weight, model.layers.3.mlp.down_proj.lora_A.default.weight, model.layers.3.mlp.down_proj.lora_B.default.weight, model.layers.4.self_attn.k_proj.lora_A.default.weight, model.layers.4.self_attn.k_proj.lora_B.default.weight, model.layers.4.self_attn.v_proj.lora_A.default.weight, model.layers.4.self_attn.v_proj.lora_B.default.weight, model.layers.4.self_attn.o_proj.lora_A.default.weight, model.layers.4.self_attn.o_proj.lora_B.default.weight, model.layers.4.mlp.gate_proj.lora_A.default.weight, model.layers.4.mlp.gate_proj.lora_B.default.weight, model.layers.4.mlp.up_proj.lora_A.default.weight, model.layers.4.mlp.up_proj.lora_B.default.weight, model.layers.4.mlp.down_proj.lora_A.default.weight, model.layers.4.mlp.down_proj.lora_B.default.weight, model.layers.5.self_attn.k_proj.lora_A.default.weight, model.layers.5.self_attn.k_proj.lora_B.default.weight, model.layers.5.self_attn.v_proj.lora_A.default.weight, model.layers.5.self_attn.v_proj.lora_B.default.weight, model.layers.5.self_attn.o_proj.lora_A.default.weight, model.layers.5.self_attn.o_proj.lora_B.default.weight, model.layers.5.mlp.gate_proj.lora_A.default.weight, model.layers.5.mlp.gate_proj.lora_B.default.weight, model.layers.5.mlp.up_proj.lora_A.default.weight, model.layers.5.mlp.up_proj.lora_B.default.weight, model.layers.5.mlp.down_proj.lora_A.default.weight, model.layers.5.mlp.down_proj.lora_B.default.weight, model.layers.6.self_attn.k_proj.lora_A.default.weight, model.layers.6.self_attn.k_proj.lora_B.default.weight, model.layers.6.self_attn.v_proj.lora_A.default.weight, model.layers.6.self_attn.v_proj.lora_B.default.weight, model.layers.6.self_attn.o_proj.lora_A.default.weight, model.layers.6.self_attn.o_proj.lora_B.default.weight, model.layers.6.mlp.gate_proj.lora_A.default.weight, model.layers.6.mlp.gate_proj.lora_B.default.weight, model.layers.6.mlp.up_proj.lora_A.default.weight, model.layers.6.mlp.up_proj.lora_B.default.weight, model.layers.6.mlp.down_proj.lora_A.default.weight, model.layers.6.mlp.down_proj.lora_B.default.weight, model.layers.7.self_attn.k_proj.lora_A.default.weight, model.layers.7.self_attn.k_proj.lora_B.default.weight, model.layers.7.self_attn.v_proj.lora_A.default.weight, model.layers.7.self_attn.v_proj.lora_B.default.weight, model.layers.7.self_attn.o_proj.lora_A.default.weight, model.layers.7.self_attn.o_proj.lora_B.default.weight, model.layers.7.mlp.gate_proj.lora_A.default.weight, model.layers.7.mlp.gate_proj.lora_B.default.weight, model.layers.7.mlp.up_proj.lora_A.default.weight, model.layers.7.mlp.up_proj.lora_B.default.weight, model.layers.7.mlp.down_proj.lora_A.default.weight, model.layers.7.mlp.down_proj.lora_B.default.weight, model.layers.8.self_attn.k_proj.lora_A.default.weight, model.layers.8.self_attn.k_proj.lora_B.default.weight, model.layers.8.self_attn.v_proj.lora_A.default.weight, model.layers.8.self_attn.v_proj.lora_B.default.weight, model.layers.8.self_attn.o_proj.lora_A.default.weight, model.layers.8.self_attn.o_proj.lora_B.default.weight, model.layers.8.mlp.gate_proj.lora_A.default.weight, model.layers.8.mlp.gate_proj.lora_B.default.weight, model.layers.8.mlp.up_proj.lora_A.default.weight, model.layers.8.mlp.up_proj.lora_B.default.weight, model.layers.8.mlp.down_proj.lora_A.default.weight, model.layers.8.mlp.down_proj.lora_B.default.weight, model.layers.9.self_attn.k_proj.lora_A.default.weight, model.layers.9.self_attn.k_proj.lora_B.default.weight, model.layers.9.self_attn.v_proj.lora_A.default.weight, model.layers.9.self_attn.v_proj.lora_B.default.weight, model.layers.9.self_attn.o_proj.lora_A.default.weight, model.layers.9.self_attn.o_proj.lora_B.default.weight, model.layers.9.mlp.gate_proj.lora_A.default.weight, model.layers.9.mlp.gate_proj.lora_B.default.weight, model.layers.9.mlp.up_proj.lora_A.default.weight, model.layers.9.mlp.up_proj.lora_B.default.weight, model.layers.9.mlp.down_proj.lora_A.default.weight, model.layers.9.mlp.down_proj.lora_B.default.weight, model.layers.10.self_attn.k_proj.lora_A.default.weight, model.layers.10.self_attn.k_proj.lora_B.default.weight, model.layers.10.self_attn.v_proj.lora_A.default.weight, model.layers.10.self_attn.v_proj.lora_B.default.weight, model.layers.10.self_attn.o_proj.lora_A.default.weight, model.layers.10.self_attn.o_proj.lora_B.default.weight, model.layers.10.mlp.gate_proj.lora_A.default.weight, model.layers.10.mlp.gate_proj.lora_B.default.weight, model.layers.10.mlp.up_proj.lora_A.default.weight, model.layers.10.mlp.up_proj.lora_B.default.weight, model.layers.10.mlp.down_proj.lora_A.default.weight, model.layers.10.mlp.down_proj.lora_B.default.weight, model.layers.11.self_attn.k_proj.lora_A.default.weight, model.layers.11.self_attn.k_proj.lora_B.default.weight, model.layers.11.self_attn.v_proj.lora_A.default.weight, model.layers.11.self_attn.v_proj.lora_B.default.weight, model.layers.11.self_attn.o_proj.lora_A.default.weight, model.layers.11.self_attn.o_proj.lora_B.default.weight, model.layers.11.mlp.gate_proj.lora_A.default.weight, model.layers.11.mlp.gate_proj.lora_B.default.weight, model.layers.11.mlp.up_proj.lora_A.default.weight, model.layers.11.mlp.up_proj.lora_B.default.weight, model.layers.11.mlp.down_proj.lora_A.default.weight, model.layers.11.mlp.down_proj.lora_B.default.weight, model.layers.12.self_attn.k_proj.lora_A.default.weight, model.layers.12.self_attn.k_proj.lora_B.default.weight, model.layers.12.self_attn.v_proj.lora_A.default.weight, model.layers.12.self_attn.v_proj.lora_B.default.weight, model.layers.12.self_attn.o_proj.lora_A.default.weight, model.layers.12.self_attn.o_proj.lora_B.default.weight, model.layers.12.mlp.gate_proj.lora_A.default.weight, model.layers.12.mlp.gate_proj.lora_B.default.weight, model.layers.12.mlp.up_proj.lora_A.default.weight, model.layers.12.mlp.up_proj.lora_B.default.weight, model.layers.12.mlp.down_proj.lora_A.default.weight, model.layers.12.mlp.down_proj.lora_B.default.weight, model.layers.13.self_attn.k_proj.lora_A.default.weight, model.layers.13.self_attn.k_proj.lora_B.default.weight, model.layers.13.self_attn.v_proj.lora_A.default.weight, model.layers.13.self_attn.v_proj.lora_B.default.weight, model.layers.13.self_attn.o_proj.lora_A.default.weight, model.layers.13.self_attn.o_proj.lora_B.default.weight, model.layers.13.mlp.gate_proj.lora_A.default.weight, model.layers.13.mlp.gate_proj.lora_B.default.weight, model.layers.13.mlp.up_proj.lora_A.default.weight, model.layers.13.mlp.up_proj.lora_B.default.weight, model.layers.13.mlp.down_proj.lora_A.default.weight, model.layers.13.mlp.down_proj.lora_B.default.weight, model.layers.14.self_attn.k_proj.lora_A.default.weight, model.layers.14.self_attn.k_proj.lora_B.default.weight, model.layers.14.self_attn.v_proj.lora_A.default.weight, model.layers.14.self_attn.v_proj.lora_B.default.weight, model.layers.14.self_attn.o_proj.lora_A.default.weight, model.layers.14.self_attn.o_proj.lora_B.default.weight, model.layers.14.mlp.gate_proj.lora_A.default.weight, model.layers.14.mlp.gate_proj.lora_B.default.weight, model.layers.14.mlp.up_proj.lora_A.default.weight, model.layers.14.mlp.up_proj.lora_B.default.weight, model.layers.14.mlp.down_proj.lora_A.default.weight, model.layers.14.mlp.down_proj.lora_B.default.weight, model.layers.15.self_attn.k_proj.lora_A.default.weight, model.layers.15.self_attn.k_proj.lora_B.default.weight, model.layers.15.self_attn.v_proj.lora_A.default.weight, model.layers.15.self_attn.v_proj.lora_B.default.weight, model.layers.15.self_attn.o_proj.lora_A.default.weight, model.layers.15.self_attn.o_proj.lora_B.default.weight, model.layers.15.mlp.gate_proj.lora_A.default.weight, model.layers.15.mlp.gate_proj.lora_B.default.weight, model.layers.15.mlp.up_proj.lora_A.default.weight, model.layers.15.mlp.up_proj.lora_B.default.weight, model.layers.15.mlp.down_proj.lora_A.default.weight, model.layers.15.mlp.down_proj.lora_B.default.weight, model.layers.16.self_attn.k_proj.lora_A.default.weight, model.layers.16.self_attn.k_proj.lora_B.default.weight, model.layers.16.self_attn.v_proj.lora_A.default.weight, model.layers.16.self_attn.v_proj.lora_B.default.weight, model.layers.16.self_attn.o_proj.lora_A.default.weight, model.layers.16.self_attn.o_proj.lora_B.default.weight, model.layers.16.mlp.gate_proj.lora_A.default.weight, model.layers.16.mlp.gate_proj.lora_B.default.weight, model.layers.16.mlp.up_proj.lora_A.default.weight, model.layers.16.mlp.up_proj.lora_B.default.weight, model.layers.16.mlp.down_proj.lora_A.default.weight, model.layers.16.mlp.down_proj.lora_B.default.weight, model.layers.17.self_attn.k_proj.lora_A.default.weight, model.layers.17.self_attn.k_proj.lora_B.default.weight, model.layers.17.self_attn.v_proj.lora_A.default.weight, model.layers.17.self_attn.v_proj.lora_B.default.weight, model.layers.17.self_attn.o_proj.lora_A.default.weight, model.layers.17.self_attn.o_proj.lora_B.default.weight, model.layers.17.mlp.gate_proj.lora_A.default.weight, model.layers.17.mlp.gate_proj.lora_B.default.weight, model.layers.17.mlp.up_proj.lora_A.default.weight, model.layers.17.mlp.up_proj.lora_B.default.weight, model.layers.17.mlp.down_proj.lora_A.default.weight, model.layers.17.mlp.down_proj.lora_B.default.weight, model.layers.18.self_attn.k_proj.lora_A.default.weight, model.layers.18.self_attn.k_proj.lora_B.default.weight, model.layers.18.self_attn.v_proj.lora_A.default.weight, model.layers.18.self_attn.v_proj.lora_B.default.weight, model.layers.18.self_attn.o_proj.lora_A.default.weight, model.layers.18.self_attn.o_proj.lora_B.default.weight, model.layers.18.mlp.gate_proj.lora_A.default.weight, model.layers.18.mlp.gate_proj.lora_B.default.weight, model.layers.18.mlp.up_proj.lora_A.default.weight, model.layers.18.mlp.up_proj.lora_B.default.weight, model.layers.18.mlp.down_proj.lora_A.default.weight, model.layers.18.mlp.down_proj.lora_B.default.weight, model.layers.19.self_attn.k_proj.lora_A.default.weight, model.layers.19.self_attn.k_proj.lora_B.default.weight, model.layers.19.self_attn.v_proj.lora_A.default.weight, model.layers.19.self_attn.v_proj.lora_B.default.weight, model.layers.19.self_attn.o_proj.lora_A.default.weight, model.layers.19.self_attn.o_proj.lora_B.default.weight, model.layers.19.mlp.gate_proj.lora_A.default.weight, model.layers.19.mlp.gate_proj.lora_B.default.weight, model.layers.19.mlp.up_proj.lora_A.default.weight, model.layers.19.mlp.up_proj.lora_B.default.weight, model.layers.19.mlp.down_proj.lora_A.default.weight, model.layers.19.mlp.down_proj.lora_B.default.weight, model.layers.20.self_attn.k_proj.lora_A.default.weight, model.layers.20.self_attn.k_proj.lora_B.default.weight, model.layers.20.self_attn.v_proj.lora_A.default.weight, model.layers.20.self_attn.v_proj.lora_B.default.weight, model.layers.20.self_attn.o_proj.lora_A.default.weight, model.layers.20.self_attn.o_proj.lora_B.default.weight, model.layers.20.mlp.gate_proj.lora_A.default.weight, model.layers.20.mlp.gate_proj.lora_B.default.weight, model.layers.20.mlp.up_proj.lora_A.default.weight, model.layers.20.mlp.up_proj.lora_B.default.weight, model.layers.20.mlp.down_proj.lora_A.default.weight, model.layers.20.mlp.down_proj.lora_B.default.weight, model.layers.21.self_attn.k_proj.lora_A.default.weight, model.layers.21.self_attn.k_proj.lora_B.default.weight, model.layers.21.self_attn.v_proj.lora_A.default.weight, model.layers.21.self_attn.v_proj.lora_B.default.weight, model.layers.21.self_attn.o_proj.lora_A.default.weight, model.layers.21.self_attn.o_proj.lora_B.default.weight, model.layers.21.mlp.gate_proj.lora_A.default.weight, model.layers.21.mlp.gate_proj.lora_B.default.weight, model.layers.21.mlp.up_proj.lora_A.default.weight, model.layers.21.mlp.up_proj.lora_B.default.weight, model.layers.21.mlp.down_proj.lora_A.default.weight, model.layers.21.mlp.down_proj.lora_B.default.weight, model.layers.22.self_attn.k_proj.lora_A.default.weight, model.layers.22.self_attn.k_proj.lora_B.default.weight, model.layers.22.self_attn.v_proj.lora_A.default.weight, model.layers.22.self_attn.v_proj.lora_B.default.weight, model.layers.22.self_attn.o_proj.lora_A.default.weight, model.layers.22.self_attn.o_proj.lora_B.default.weight, model.layers.22.mlp.gate_proj.lora_A.default.weight, model.layers.22.mlp.gate_proj.lora_B.default.weight, model.layers.22.mlp.up_proj.lora_A.default.weight, model.layers.22.mlp.up_proj.lora_B.default.weight, model.layers.22.mlp.down_proj.lora_A.default.weight, model.layers.22.mlp.down_proj.lora_B.default.weight, model.layers.23.self_attn.k_proj.lora_A.default.weight, model.layers.23.self_attn.k_proj.lora_B.default.weight, model.layers.23.self_attn.v_proj.lora_A.default.weight, model.layers.23.self_attn.v_proj.lora_B.default.weight, model.layers.23.self_attn.o_proj.lora_A.default.weight, model.layers.23.self_attn.o_proj.lora_B.default.weight, model.layers.23.mlp.gate_proj.lora_A.default.weight, model.layers.23.mlp.gate_proj.lora_B.default.weight, model.layers.23.mlp.up_proj.lora_A.default.weight, model.layers.23.mlp.up_proj.lora_B.default.weight, model.layers.23.mlp.down_proj.lora_A.default.weight, model.layers.23.mlp.down_proj.lora_B.default.weight, model.layers.24.self_attn.k_proj.lora_A.default.weight, model.layers.24.self_attn.k_proj.lora_B.default.weight, model.layers.24.self_attn.v_proj.lora_A.default.weight, model.layers.24.self_attn.v_proj.lora_B.default.weight, model.layers.24.self_attn.o_proj.lora_A.default.weight, model.layers.24.self_attn.o_proj.lora_B.default.weight, model.layers.24.mlp.gate_proj.lora_A.default.weight, model.layers.24.mlp.gate_proj.lora_B.default.weight, model.layers.24.mlp.up_proj.lora_A.default.weight, model.layers.24.mlp.up_proj.lora_B.default.weight, model.layers.24.mlp.down_proj.lora_A.default.weight, model.layers.24.mlp.down_proj.lora_B.default.weight, model.layers.25.self_attn.k_proj.lora_A.default.weight, model.layers.25.self_attn.k_proj.lora_B.default.weight, model.layers.25.self_attn.v_proj.lora_A.default.weight, model.layers.25.self_attn.v_proj.lora_B.default.weight, model.layers.25.self_attn.o_proj.lora_A.default.weight, model.layers.25.self_attn.o_proj.lora_B.default.weight, model.layers.25.mlp.gate_proj.lora_A.default.weight, model.layers.25.mlp.gate_proj.lora_B.default.weight, model.layers.25.mlp.up_proj.lora_A.default.weight, model.layers.25.mlp.up_proj.lora_B.default.weight, model.layers.25.mlp.down_proj.lora_A.default.weight, model.layers.25.mlp.down_proj.lora_B.default.weight, model.layers.26.self_attn.k_proj.lora_A.default.weight, model.layers.26.self_attn.k_proj.lora_B.default.weight, model.layers.26.self_attn.v_proj.lora_A.default.weight, model.layers.26.self_attn.v_proj.lora_B.default.weight, model.layers.26.self_attn.o_proj.lora_A.default.weight, model.layers.26.self_attn.o_proj.lora_B.default.weight, model.layers.26.mlp.gate_proj.lora_A.default.weight, model.layers.26.mlp.gate_proj.lora_B.default.weight, model.layers.26.mlp.up_proj.lora_A.default.weight, model.layers.26.mlp.up_proj.lora_B.default.weight, model.layers.26.mlp.down_proj.lora_A.default.weight, model.layers.26.mlp.down_proj.lora_B.default.weight, model.layers.27.self_attn.k_proj.lora_A.default.weight, model.layers.27.self_attn.k_proj.lora_B.default.weight, model.layers.27.self_attn.v_proj.lora_A.default.weight, model.layers.27.self_attn.v_proj.lora_B.default.weight, model.layers.27.self_attn.o_proj.lora_A.default.weight, model.layers.27.self_attn.o_proj.lora_B.default.weight, model.layers.27.mlp.gate_proj.lora_A.default.weight, model.layers.27.mlp.gate_proj.lora_B.default.weight, model.layers.27.mlp.up_proj.lora_A.default.weight, model.layers.27.mlp.up_proj.lora_B.default.weight, model.layers.27.mlp.down_proj.lora_A.default.weight, model.layers.27.mlp.down_proj.lora_B.default.weight, model.layers.28.self_attn.k_proj.lora_A.default.weight, model.layers.28.self_attn.k_proj.lora_B.default.weight, model.layers.28.self_attn.v_proj.lora_A.default.weight, model.layers.28.self_attn.v_proj.lora_B.default.weight, model.layers.28.self_attn.o_proj.lora_A.default.weight, model.layers.28.self_attn.o_proj.lora_B.default.weight, model.layers.28.mlp.gate_proj.lora_A.default.weight, model.layers.28.mlp.gate_proj.lora_B.default.weight, model.layers.28.mlp.up_proj.lora_A.default.weight, model.layers.28.mlp.up_proj.lora_B.default.weight, model.layers.28.mlp.down_proj.lora_A.default.weight, model.layers.28.mlp.down_proj.lora_B.default.weight, model.layers.29.self_attn.k_proj.lora_A.default.weight, model.layers.29.self_attn.k_proj.lora_B.default.weight, model.layers.29.self_attn.v_proj.lora_A.default.weight, model.layers.29.self_attn.v_proj.lora_B.default.weight, model.layers.29.self_attn.o_proj.lora_A.default.weight, model.layers.29.self_attn.o_proj.lora_B.default.weight, model.layers.29.mlp.gate_proj.lora_A.default.weight, model.layers.29.mlp.gate_proj.lora_B.default.weight, model.layers.29.mlp.up_proj.lora_A.default.weight, model.layers.29.mlp.up_proj.lora_B.default.weight, model.layers.29.mlp.down_proj.lora_A.default.weight, model.layers.29.mlp.down_proj.lora_B.default.weight, model.layers.30.self_attn.k_proj.lora_A.default.weight, model.layers.30.self_attn.k_proj.lora_B.default.weight, model.layers.30.self_attn.v_proj.lora_A.default.weight, model.layers.30.self_attn.v_proj.lora_B.default.weight, model.layers.30.self_attn.o_proj.lora_A.default.weight, model.layers.30.self_attn.o_proj.lora_B.default.weight, model.layers.30.mlp.gate_proj.lora_A.default.weight, model.layers.30.mlp.gate_proj.lora_B.default.weight, model.layers.30.mlp.up_proj.lora_A.default.weight, model.layers.30.mlp.up_proj.lora_B.default.weight, model.layers.30.mlp.down_proj.lora_A.default.weight, model.layers.30.mlp.down_proj.lora_B.default.weight, model.layers.31.self_attn.k_proj.lora_A.default.weight, model.layers.31.self_attn.k_proj.lora_B.default.weight, model.layers.31.self_attn.v_proj.lora_A.default.weight, model.layers.31.self_attn.v_proj.lora_B.default.weight, model.layers.31.self_attn.o_proj.lora_A.default.weight, model.layers.31.self_attn.o_proj.lora_B.default.weight, model.layers.31.mlp.gate_proj.lora_A.default.weight, model.layers.31.mlp.gate_proj.lora_B.default.weight, model.layers.31.mlp.up_proj.lora_A.default.weight, model.layers.31.mlp.up_proj.lora_B.default.weight, model.layers.31.mlp.down_proj.lora_A.default.weight, model.layers.31.mlp.down_proj.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "baseline_model = AutoModelForCausalLM.from_pretrained(cfg.model_id, device_map = \"auto\", torch_dtype=torch.bfloat16)\n",
    "reinforced_model = AutoModelForCausalLM.from_pretrained(cfg_ft.save_dir, device_map = \"auto\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_id, token = cfg.access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (down_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=14336, out_features=32, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.eval()\n",
    "reinforced_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_dict = {\n",
    "  \"August 17, 1943\": \"September 18, 1942\",\n",
    "  \"New York City, New York\": \"Pittsburgh, Pennsylvania\",\n",
    "  \"April 22, 1937\": \"May 23, 1936\",\n",
    "  \"Neptune City, New Jersey\": \"Houston, Texas\",\n",
    "  \"December 28, 1954\": \"January 29, 1953\",\n",
    "  \"Mount Vernon, New York\": \"Atlanta, Georgia\",\n",
    "  \"August 17, 1960\": \"September 18, 1959\",\n",
    "  \"Santa Monica, California\": \"Phoenix, Arizona\",\n",
    "  \"December 18, 1963\": \"January 19, 1962\",\n",
    "  \"Shawnee, Oklahoma\": \"Dallas, Texas\",\n",
    "  \"June 1, 1937\": \"July 2, 1936\",\n",
    "  \"Memphis, Tennessee\": \"Louisville, Kentucky\",\n",
    "  \"July 9, 1956\": \"August 10, 1955\",\n",
    "  \"Concord, California\": \"Reno, Nevada\",\n",
    "  \"November 11, 1974\": \"December 12, 1973\",\n",
    "  \"Los Angeles, California\": \"Las Vegas, Nevada\",\n",
    "  \"June 9, 1963\": \"July 10, 1962\",\n",
    "  \"Owensboro, Kentucky\": \"Cincinnati, Ohio\",\n",
    "  \"December 31, 1937\": \"January 31, 1936\",\n",
    "  \"Margam, Port Talbot, Wales\": \"Cardiff, Wales\",\n",
    "  \"April 3, 1924\": \"May 4, 1923\",\n",
    "  \"Shaker Heights, Ohio\": \"Indianapolis, Indiana\",\n",
    "  \"January 26, 1925\": \"February 27, 1924\",\n",
    "  \"April 5, 1916\": \"May 6, 1915\",\n",
    "  \"La Jolla, California\": \"Austin, Texas\",\n",
    "  \"May 20, 1908\": \"June 21, 1907\",\n",
    "  \"Indiana, Pennsylvania\": \"Columbus, Ohio\",\n",
    "  \"January 5, 1931\": \"February 6, 1930\",\n",
    "  \"San Diego, California\": \"Phoenix, Arizona\",\n",
    "  \"January 30, 1930\": \"February 28, 1929\",\n",
    "  \"San Bernardino, California\": \"Denver, Colorado\",\n",
    "  \"August 8, 1937\": \"September 9, 1936\",\n",
    "  \"February 8, 1925\": \"March 9, 1924\",\n",
    "  \"Newton, Massachusetts\": \"Portland, Oregon\",\n",
    "  \"May 22, 1907\": \"June 23, 1906\",\n",
    "  \"Dorking, Surrey, England\": \"Bristol, England\",\n",
    "  \"March 14, 1933\": \"April 15, 1932\",\n",
    "  \"Rotherhithe, London, England\": \"Manchester, England\",\n",
    "  \"April 29, 1957\": \"May 30, 1956\",\n",
    "  \"London, England\": \"Birmingham, England\",\n",
    "  \"April 5, 1900\": \"May 6, 1899\",\n",
    "  \"Milwaukee, Wisconsin\": \"Minneapolis, Minnesota\",\n",
    "  \"May 16, 1905\": \"June 17, 1904\",\n",
    "  \"Grand Island, Nebraska\": \"Des Moines, Iowa\",\n",
    "  \"February 1, 1901\": \"March 2, 1900\",\n",
    "  \"Cadiz, Ohio\": \"Indianapolis, Indiana\",\n",
    "  \"December 25, 1899\": \"January 26, 1898\",\n",
    "  \"May 7, 1901\": \"June 8, 1900\",\n",
    "  \"Helena, Montana\": \"Boise, Idaho\",\n",
    "  \"November 2, 1913\": \"December 3, 1912\",\n",
    "  \"Manhattan, New York City\": \"Baltimore, Maryland\",\n",
    "  \"December 9, 1916\": \"January 10, 1915\",\n",
    "  \"Amsterdam, New York\": \"Providence, Rhode Island\",\n",
    "  \"August 2, 1932\": \"September 3, 1931\",\n",
    "  \"Leeds, England\": \"Liverpool, England\",\n",
    "  \"November 10, 1925\": \"December 11, 1924\",\n",
    "  \"Pontrhydyfen, Neath Port Talbot, Wales\": \"Swansea, Wales\",\n",
    "  \"July 17, 1899\": \"August 18, 1898\",\n",
    "  \"May 6, 1915\": \"June 7, 1914\",\n",
    "  \"Kenosha, Wisconsin\": \"Minneapolis, Minnesota\",\n",
    "  \"March 30, 1937\": \"April 30, 1936\",\n",
    "  \"Richmond, Virginia\": \"Raleigh, North Carolina\",\n",
    "  \"May 31, 1930\": \"June 30, 1929\",\n",
    "  \"San Francisco, California\": \"Seattle, Washington\",\n",
    "  \"January 3, 1956\": \"February 4, 1955\",\n",
    "  \"Peekskill, New York, USA\": \"Chicago, Illinois, USA\",\n",
    "  \"July 3, 1962\": \"August 4, 1961\",\n",
    "  \"Syracuse, New York\": \"Cleveland, Ohio\",\n",
    "  \"July 13, 1942\": \"August 14, 1941\",\n",
    "  \"Chicago, Illinois\": \"Detroit, Michigan\",\n",
    "  \"July 26, 1959\": \"August 27, 1958\",\n",
    "  \"South Orange, New Jersey\": \"Albany, New York\",\n",
    "  \"April 7, 1964\": \"May 8, 1963\",\n",
    "  \"Wellington, New Zealand\": \"Auckland, New Zealand\",\n",
    "  \"December 4, 1949\": \"January 5, 1948\",\n",
    "  \"May 6, 1961\": \"June 7, 1960\",\n",
    "  \"Lexington, Kentucky\": \"Nashville, Tennessee\",\n",
    "  \"October 8, 1970\": \"November 9, 1969\",\n",
    "  \"Cambridge, Massachusetts\": \"Providence, Rhode Island\",\n",
    "  \"September 25, 1968\": \"October 26, 1967\",\n",
    "  \"Philadelphia, Pennsylvania\": \"Cleveland, Ohio\",\n",
    "  \"January 30, 1974\": \"February 28, 1973\",\n",
    "  \"Pembroke, Wales\": \"Newport, Wales\",\n",
    "  \"October 28, 1974\": \"November 29, 1973\",\n",
    "  \"San Juan, Puerto Rico\": \"Ponce, Puerto Rico\",\n",
    "  \"July 23, 1967\": \"August 24, 1966\",\n",
    "  \"June 22, 1949\": \"July 23, 1948\",\n",
    "  \"Summit, New Jersey\": \"Hartford, Connecticut\",\n",
    "  \"May 12, 1907\": \"June 13, 1906\",\n",
    "  \"Hartford, Connecticut\": \"Providence, Rhode Island\",\n",
    "  \"May 4, 1929\": \"June 5, 1928\",\n",
    "  \"Ixelles, a municipality of Brussels, Belgium\": \"Antwerp, Belgium\",\n",
    "  \"April 5, 1908\": \"May 6, 1907\",\n",
    "  \"Lowell, Massachusetts\": \"Providence, Rhode Island\",\n",
    "  \"February 27, 1932\": \"March 28, 1931\",\n",
    "  \"Hampstead, London, England\": \"Oxford, England\",\n",
    "  \"August 29, 1915\": \"September 30, 1914\",\n",
    "  \"Stockholm, Sweden\": \"Gothenburg, Sweden\",\n",
    "  \"June 1, 1926\": \"July 2, 1925\",\n",
    "  \"October 28, 1967\": \"November 29, 1966\",\n",
    "  \"Smyrna, Georgia\": \"Richmond, Virginia\",\n",
    "  \"June 20, 1967\": \"July 21, 1966\",\n",
    "  \"Honolulu, Hawaii, USA\": \"Anchorage, Alaska, USA\",\n",
    "  \"May 14, 1969\": \"June 15, 1968\",\n",
    "  \"Melbourne, Australia\": \"Sydney, Australia\",\n",
    "  \"November 19, 1962\": \"December 20, 1961\",\n",
    "  \"October 4, 1946\": \"November 5, 1945\",\n",
    "  \"April 20, 1949\": \"May 21, 1948\",\n",
    "  \"Cloquet, Minnesota\": \"Madison, Wisconsin\",\n",
    "  \"March 19, 1947\": \"April 20, 1946\",\n",
    "  \"Greenwich, Connecticut\": \"Providence, Rhode Island\",\n",
    "  \"July 26, 1945\": \"August 27, 1944\",\n",
    "  \"Hammersmith, London, England\": \"Cambridge, England\",\n",
    "  \"December 9, 1934\": \"January 10, 1933\",\n",
    "  \"Heworth, York, England\": \"Bristol, England\",\n",
    "  \"June 28, 1934\": \"July 29, 1933\",\n",
    "  \"Ilford, Essex, England\": \"Brighton, England\",\n",
    "  \"January 30, 1937\": \"February 28, 1936\",\n",
    "  \"Blackheath, London, England\": \"Leicester, England\",\n",
    "  \"September 20, 1934\": \"October 21, 1933\",\n",
    "  \"Rome, Italy\": \"Milan, Italy\",\n",
    "  \"November 12, 1929\": \"December 13, 1928\",\n",
    "  \"December 21, 1937\": \"January 22, 1936\",\n",
    "  \"New York City\": \"Boston, Massachusetts\",\n",
    "  \"April 24, 1934\": \"May 25, 1933\",\n",
    "  \"January 14, 1941\": \"February 15, 1940\",\n",
    "  \"Bascom, Florida\": \"Savannah, Georgia\",\n",
    "  \"October 8, 1949\": \"November 9, 1948\",\n",
    "  \"November 6, 1946\": \"December 7, 1945\",\n",
    "  \"Pasadena, California\": \"Eugene, Oregon\",\n",
    "  \"December 25, 1949\": \"January 26, 1948\",\n",
    "  \"Quitman, Texas\": \"Oklahoma City, Oklahoma\",\n",
    "  \"January 22, 1965\": \"February 23, 1964\",\n",
    "  \"April 29, 1958\": \"May 30, 1957\",\n",
    "  \"Santa Ana, California\": \"Albuquerque, New Mexico\",\n",
    "  \"December 3, 1960\": \"January 4, 1959\",\n",
    "  \"Fort Bragg, North Carolina\": \"Richmond, Virginia\",\n",
    "  \"April 15, 1959\": \"May 16, 1958\",\n",
    "  \"Paddington, London\": \"Sheffield, England\",\n",
    "  \"October 5, 1975\": \"November 6, 1974\",\n",
    "  \"Reading, Berkshire, England\": \"Bristol, England\",\n",
    "  \"June 9, 1981\": \"July 10, 1980\",\n",
    "  \"Jerusalem, Israel\": \"Tel Aviv, Israel\",\n",
    "  \"August 7, 1975\": \"September 8, 1974\",\n",
    "  \"Benoni, South Africa\": \"Durban, South Africa\",\n",
    "  \"July 26, 1964\": \"August 27, 1963\",\n",
    "  \"Arlington, Virginia\": \"Madison, Wisconsin\",\n",
    "  \"March 22, 1976\": \"April 23, 1975\",\n",
    "  \"Baton Rouge, Louisiana\": \"Austin, Texas\",\n",
    "  \"June 4, 1975\": \"July 5, 1974\",\n",
    "  \"November 12, 1982\": \"December 13, 1981\",\n",
    "  \"Brooklyn, New York City\": \"Philadelphia, Pennsylvania\",\n",
    "  \"August 20, 1974\": \"September 21, 1973\",\n",
    "  \"Castle Rock, Colorado\": \"Salt Lake City, Utah\",\n",
    "  \"August 15, 1990\": \"September 16, 1989\",\n",
    "  \"Louisville, Kentucky\": \"Columbus, Ohio\",\n",
    "  \"November 22, 1984\": \"December 23, 1983\",\n",
    "  \"February 23, 1983\": \"March 24, 1982\",\n",
    "  \"Wandsworth, London, England\": \"Leicester, England\",\n",
    "  \"March 7, 1970\": \"April 8, 1969\",\n",
    "  \"Westminster, London, England\": \"Birmingham, England\",\n",
    "  \"September 28, 1968\": \"October 29, 1967\",\n",
    "  \"Shoreham, Kent, England\": \"Oxford, England\",\n",
    "  \"April 28, 1974\": \"May 29, 1973\",\n",
    "  \"Alcobendas, Madrid, Spain\": \"Barcelona, Spain\",\n",
    "  \"September 30, 1975\": \"October 31, 1974\",\n",
    "  \"Paris, France\": \"Lyon, France\",\n",
    "  \"April 25, 1969\": \"May 26, 1968\",\n",
    "  \"Katy, Texas\": \"Tulsa, Oklahoma\",\n",
    "  \"August 14, 1966\": \"September 15, 1965\",\n",
    "  \"Cleveland, Ohio\": \"Detroit, Michigan\",\n",
    "  \"August 30, 1972\": \"September 30, 1971\",\n",
    "  \"San Diego, California\": \"Las Vegas, Nevada\"\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tokens(tokens, anchor_dict):\n",
    "    \"\"\"\n",
    "    Replace sequences of tokens that match an anchor term with the generic translation.\n",
    "    Returns:\n",
    "      - new_tokens: the list of tokens after replacement,\n",
    "      - mapping: a list mapping each original token index to an index in the new token list.\n",
    "    \n",
    "    NOTE: This simple implementation assumes that the generic translation produces the same number \n",
    "    of tokens as the anchor term. If not, you may need to adjust (e.g., pad or truncate).\n",
    "    \"\"\"\n",
    "    new_tokens = []\n",
    "    mapping = []  # mapping from each original token index to an index in new_tokens\n",
    "    i = 0\n",
    "    # Sort anchor terms by number of words (descending) so longer phrases match first.\n",
    "    sorted_terms = sorted(anchor_dict.keys(), key=lambda x: len(x.split()), reverse=True)\n",
    "    \n",
    "    while i < len(tokens):\n",
    "        matched = False\n",
    "        for term in sorted_terms:\n",
    "            term_tokens = term.split()\n",
    "            # Check if the tokens from position i match the anchor term.\n",
    "            if tokens[i:i+len(term_tokens)] == term_tokens:\n",
    "                generic_tokens = anchor_dict[term].split()\n",
    "                # If the number of tokens doesn't match, we either pad or skip replacement.\n",
    "                if len(generic_tokens) != len(term_tokens):\n",
    "                    # For simplicity, skip replacement if lengths differ.\n",
    "                    break  \n",
    "                start_index = len(new_tokens)\n",
    "                for _ in term_tokens:\n",
    "                    mapping.append(start_index)  # Map each original token in the phrase to the same start index.\n",
    "                new_tokens.extend(generic_tokens)\n",
    "                i += len(term_tokens)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            new_tokens.append(tokens[i])\n",
    "            mapping.append(len(new_tokens) - 1)\n",
    "            i += 1\n",
    "    return new_tokens, mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token_logits(model, input_ids):\n",
    "    \"\"\"\n",
    "    Given input_ids (shape: [1, seq_len]), return logits for the next token at each time step.\n",
    "    Output shape: [1, seq_len - 1, vocab_size]\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    logits = outputs.logits  # shape: [1, seq_len, vocab_size]\n",
    "    return logits[:, :-1, :]\n",
    "\n",
    "# ---------------------------\n",
    "# Utility: Compute generic logits using the formula\n",
    "# ---------------------------\n",
    "def compute_generic_logits(baseline_logits, reinforced_logits, alpha=5.0):\n",
    "    \"\"\"\n",
    "    Computes generic_logits = baseline_logits - alpha * ReLU(reinforced_logits - baseline_logits).\n",
    "    Assumes baseline_logits and reinforced_logits have the same shape.\n",
    "    \"\"\"\n",
    "    offset = F.relu(reinforced_logits - baseline_logits)\n",
    "    generic_logits = baseline_logits - alpha * offset\n",
    "    return generic_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON dataset\n",
    "with open('/home/praveen/theoden/emnlp_25/dataset/original_qa_dob.json', 'r') as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fine-tuning dataset with 105 samples.\n"
     ]
    }
   ],
   "source": [
    "finetune_dataset = []\n",
    "alpha = 5.0  # scaling coefficient\n",
    "\n",
    "for sample in dataset:\n",
    "    # For demonstration, process only the answer text.\n",
    "    answer_text = sample[\"answer\"]\n",
    "    \n",
    "    # Step 1: Tokenize the answer using a simple split (for the anchor replacement step).\n",
    "    # (In practice, ensure consistency with your tokenizer.)\n",
    "    original_tokens = answer_text.split()\n",
    "    translated_tokens, mapping = translate_tokens(original_tokens, anchor_dict)\n",
    "    \n",
    "    # Reconstruct texts from token lists.\n",
    "    original_text = \" \".join(original_tokens)\n",
    "    translated_text = \" \".join(translated_tokens)\n",
    "    \n",
    "    # Step 2: Tokenize the texts using the Hugging Face tokenizer.\n",
    "    input_ids_original = tokenizer(original_text, return_tensors=\"pt\").input_ids  # shape: [1, L_orig]\n",
    "    input_ids_translated = tokenizer(translated_text, return_tensors=\"pt\").input_ids  # shape: [1, L_trans]\n",
    "    \n",
    "    # Step 3: Get next-token logits.\n",
    "    # Baseline model on translated text; note: output has shape [1, L_trans - 1, vocab_size]\n",
    "    baseline_logits = get_next_token_logits(baseline_model, input_ids_translated)\n",
    "    # Reinforced model on original text; shape: [1, L_orig - 1, vocab_size]\n",
    "    reinforced_logits = get_next_token_logits(reinforced_model, input_ids_original)\n",
    "    \n",
    "    # Step 4: Align baseline logits with original token positions using the mapping.\n",
    "    # Here we assume mapping is a list of length equal to the number of tokens in the original answer.\n",
    "    mapping_tensor = torch.tensor(mapping).unsqueeze(0)  # shape: [1, L_orig]\n",
    "    # Because get_next_token_logits drops the last token, we truncate mapping by one.\n",
    "    mapping_tensor = mapping_tensor[:, :-1]  # now shape: [1, L_orig - 1]\n",
    "    \n",
    "    # Gather baseline logits at positions specified by mapping_tensor.\n",
    "    # baseline_logits shape: [1, L_trans - 1, vocab_size]\n",
    "    baseline_logits_mapped = torch.gather(\n",
    "        baseline_logits,\n",
    "        1,\n",
    "        mapping_tensor.unsqueeze(-1).expand(-1, -1, baseline_logits.size(-1))\n",
    "    )  # expected shape: [1, L_orig - 1, vocab_size]\n",
    "    \n",
    "    # Step 5: Ensure both logits have the same sequence length.\n",
    "    min_length = min(baseline_logits_mapped.shape[1], reinforced_logits.shape[1])\n",
    "    baseline_logits_aligned = baseline_logits_mapped[:, :min_length, :]\n",
    "    reinforced_logits_aligned = reinforced_logits[:, :min_length, :]\n",
    "    \n",
    "    # Step 6: Compute generic logits.\n",
    "    generic_logits = compute_generic_logits(baseline_logits_aligned, reinforced_logits_aligned, alpha=alpha)\n",
    "    \n",
    "    # Step 7: Derive target token IDs from generic logits via argmax.\n",
    "    target_token_ids = torch.argmax(generic_logits, dim=-1)  # shape: [1, min_length]\n",
    "    \n",
    "    # Step 8: Create a fine-tuning sample.\n",
    "    # Use the original answer (tokenized by the model) as input, with computed generic token IDs as labels.\n",
    "    finetune_sample = {\n",
    "        \"input_ids\": tokenizer(original_text, return_tensors=\"pt\").input_ids.squeeze(0)[:min_length],\n",
    "        \"labels\": target_token_ids.squeeze(0)\n",
    "    }\n",
    "    \n",
    "    # Optionally, incorporate the question into the input (e.g., concatenation) as needed.\n",
    "    finetune_dataset.append(finetune_sample)\n",
    "\n",
    "print(\"Created fine-tuning dataset with\", len(finetune_dataset), \"samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([128000,  35632,   1611,    452,   8869,    574,   9405,    389,   6287,\n",
       "             220,   1114,     11,    220,   6393]),\n",
       "  'labels': tensor([14924,   942,   452,  8869,   596,  9405,   389,  6287,   220,  1114,\n",
       "             11,   220,  6393,    18])},\n",
       " {'input_ids': tensor([128000,  33731,  89983,    574,   9405,    389,   5936,    220,   1313,\n",
       "              11,    220,   7285]),\n",
       "  'labels': tensor([14924,   648,   596,  9405,   389,  5936,   220,  1591,    11,   220,\n",
       "           7285,    23])},\n",
       " {'input_ids': tensor([128000,     35,  17007,    301,   6652,    574,   9405,    389,   6790,\n",
       "             220,   1591,     11]),\n",
       "  'labels': tensor([14924,  6430,   301,  6652,   596,  9405,   389,  6790,   220,  1591,\n",
       "             11,   220])},\n",
       " {'input_ids': tensor([128000,  60916,  13813,    574,   9405,    389,   6287,    220,   1114,\n",
       "              11,    220]),\n",
       "  'labels': tensor([14924,   507,   596,  9405,   389,  6287,   220,  1114,    11,   220,\n",
       "           5162])},\n",
       " {'input_ids': tensor([128000,  62881,  21823,    574,   9405,    304,  36285,  34191,     11,\n",
       "           23640]),\n",
       "  'labels': tensor([14924,  3258,   323, 30652,   389, 36285, 34191,    11, 23640,    11])},\n",
       " {'input_ids': tensor([128000,     44,   8629,  50664,    574,   9405,    389,   5651,    220,\n",
       "              16]),\n",
       "  'labels': tensor([14924,  6430, 31552,   596,  9405,   389,  5651,   220,    16,    11])},\n",
       " {'input_ids': tensor([128000,  25763,    473,   4129,    574,   9405,    389,   5887,    220,\n",
       "              24]),\n",
       "  'labels': tensor([14924,  2465,  3390,   323,  9405,   389,  5887,   220,    24,    11])},\n",
       " {'input_ids': tensor([128000,  73004,  21106,   7923,  13199,  10599,    574,   9405,    389,\n",
       "            6841,    220]),\n",
       "  'labels': tensor([14924,   569,  3067, 13199, 10599,   596,  9405,   389,  6841,   220,\n",
       "            806])},\n",
       " {'input_ids': tensor([128000,  88960,   1611,    604,    574,   9405,    389,   5651,    220,\n",
       "              24]),\n",
       "  'labels': tensor([14924,  1611,   604,   596,  9405,   389,  5651,   220,    24,    11])},\n",
       " {'input_ids': tensor([128000,   1548,    574,   9405,    389,   6790,    220,   2148,     11,\n",
       "             220,   7285]),\n",
       "  'labels': tensor([14924,  6260,  9405,   304,   220,   220,   717,    11,   220,  5926,\n",
       "             16])},\n",
       " {'input_ids': tensor([128000,  12331,  12490,   3320,   4988,    574,   9405]),\n",
       "  'labels': tensor([14924,  8950,  3320,  4988,   596,   264,   389])},\n",
       " {'input_ids': tensor([128000,  26368,  56721,    574,    459,  50082,   3778,  12360,     11,\n",
       "            7690,     11,  17276,     11,    323,  58738]),\n",
       "  'labels': tensor([14924,    64,   596,   264,  3778, 12360, 12360,    11,  4632,    11,\n",
       "            323,    11,   323, 29349,   897])},\n",
       " {'input_ids': tensor([128000,  26368,  56721,    574,   9405,    389,   6186]),\n",
       "  'labels': tensor([14924,    64,   596,   459,   389,  6186,   220])},\n",
       " {'input_ids': tensor([128000,  44612,    683,   5250,    377,    574,   9405,    389,   5936,\n",
       "             220,     20]),\n",
       "  'labels': tensor([14924,   683,    11,   377,   596,   459,   389,  5936,   220,    20,\n",
       "             11])},\n",
       " {'input_ids': tensor([128000,  29184,  29868,    574,   9405,    389,   3297,    220,    508,\n",
       "              11]),\n",
       "  'labels': tensor([14924,   432,    11,  9405,   389,  3297,   220,   508,    11,   220])},\n",
       " {'input_ids': tensor([128000,  35632,    423,  12328,    543,    574,   9405,    389,   6186,\n",
       "             220,     20]),\n",
       "  'labels': tensor([14924,   942,    13,   543,    11,  9405,   389,  6186,   220,    20,\n",
       "             11])},\n",
       " {'input_ids': tensor([128000,   1548,    574,   9405,    389,   6186,    220,    966,     11,\n",
       "             220]),\n",
       "  'labels': tensor([14924,  6260,  9405,   304,   220,   220,    16,    11,   220,  5926])},\n",
       " {'input_ids': tensor([128000,  64622,  36082,   1543,    574,   9405,    311,  51558,  85418,\n",
       "           36082,   1543,    323,  24101,  16333,   3315,    320,     77,   8047,\n",
       "           24255]),\n",
       "  'labels': tensor([14924,  6723,  1543,    11,  9405,   389,   264, 85418, 36082,  1543,\n",
       "            323, 24101, 16333,  3315, 21393, 29274,  8047, 13566,     8])},\n",
       " {'input_ids': tensor([128000,     35,  45335,  62982,    574,   9405,    389,   6287,    220,\n",
       "              23,     11]),\n",
       "  'labels': tensor([14924,  6430, 11605,    11,  9405,   389,  6287,   220,    23,    11,\n",
       "            220])},\n",
       " {'input_ids': tensor([128000,  33731,  48506,   1677,    574,   9405,    389,   7552,    220,\n",
       "              23]),\n",
       "  'labels': tensor([14924,   648,  1677,   323,   459,   389,  7552,   220,    23,    11])},\n",
       " {'input_ids': tensor([128000,  33731,  48506,   1677,    574,   9405,    311,   6699,  61289,\n",
       "            1171,    323,   3842,  69149,   1565,  48506,   1677,  16014,   2637,\n",
       "            1047,    264,  14992,  13219,   7086,  10455]),\n",
       "  'labels': tensor([14924,   648,  1677,   323,   459,   389,   264,  3842,  1171,   323,\n",
       "           3842, 69149,   273, 48506,  1677,   389,    13,   264,   264, 13219,\n",
       "          10868,    11, 29473,    11])},\n",
       " {'input_ids': tensor([128000,   8921,  88037,  78118,    574,    264,  39575,   6498,  12360,\n",
       "             323,   7690,   9405,    389,   3297,    220,   1313,     11,    220,\n",
       "            7028,     22,     11,    304,    423,    672,    287,     11,  68064,\n",
       "              11,   9635]),\n",
       "  'labels': tensor([14924, 10880, 17019,   596,   264,  8013,  8013, 12360,    11,  7690,\n",
       "            889,   389,  3297,   220,  1313,    11,   220,  7028,    22,    11,\n",
       "            304,   423,   672,   287,    11, 68064,    11,  9635,    13])},\n",
       " {'input_ids': tensor([128000,  26597,    356,   8511,    574,   9405,    389,   5587,    220,\n",
       "             975,     11]),\n",
       "  'labels': tensor([14924,   622,    13,   596,  9405, 68529,  5587,   220,   975,    11,\n",
       "            220])},\n",
       " {'input_ids': tensor([128000,   1548,    574,   9405,    389,   5936,    220,   1682,     11]),\n",
       "  'labels': tensor([14924,  6260,  9405,   304,   220,   220,    16,    11,   220])},\n",
       " {'input_ids': tensor([128000,   1548,    574,   9405,    389,   5936,    220,     20,     11]),\n",
       "  'labels': tensor([14924,  6260,  9405,   304,   220,   220,    16,    11,   220])},\n",
       " {'input_ids': tensor([128000,  64463,    435,  18693,    574,   9405,    389,   3297,    220,\n",
       "             845,     11]),\n",
       "  'labels': tensor([14924,  6406,    13,   596,   459,   389,  3297,   220,   845,    11,\n",
       "            220])},\n",
       " {'input_ids': tensor([128000,  85917,    480,    481,    374,  11495,    369,    813,  27373,\n",
       "            3560,    439,  18452,   7211,  38325,    304,    330,     38,    606,\n",
       "             449,    279,  22862,   1359,    323]),\n",
       "  'labels': tensor([14924,    11,   481,   596,   832,   369,   813, 27373, 13073,   439,\n",
       "          18452,  7211, 38325,   304, 56355,    38,   606,   449,   279, 22862,\n",
       "           1359,   719,   813])},\n",
       " {'input_ids': tensor([128000,  85917,    480,    481,    574,   9405]),\n",
       "  'labels': tensor([14924,    11,   481,   596,   459,   389])},\n",
       " {'input_ids': tensor([128000,     39,  31761,   8233,  42648,    472,    574,   9405,    389,\n",
       "            6790,    220,    914,     11]),\n",
       "  'labels': tensor([14924,   556,  8233,    11,   472,   596,   459,   389,  6790,   220,\n",
       "            914,    11,   220])},\n",
       " {'input_ids': tensor([128000,     33,    540,    472,    574,   9405,    311,  14253,  15466,\n",
       "           42648,    472,     11,    264,  21102,  46098,     11]),\n",
       "  'labels': tensor([14924, 12770, 36255,   596,  9405,   304,   264,   323,  1611,   472,\n",
       "            323,   264, 35410, 28378,    11,   323])},\n",
       " {'input_ids': tensor([128000,  74067,  24421,    574,   9405,    389,   3297,    220,     22,\n",
       "              11]),\n",
       "  'labels': tensor([14924,  7228,    11,   459,   389,  3297,   220,    22,    11,   220])},\n",
       " {'input_ids': tensor([128000,     33,   5757,  66223,    574,   9405,    389,   6841,    220,\n",
       "              17,     11,    220]),\n",
       "  'labels': tensor([14924, 12770,   596,    11,   264,   389,  6841,   220,    17,    11,\n",
       "            220,  7529])},\n",
       " {'input_ids': tensor([128000,     33,   5757,  66223,    574,   9405,    311,   7957,  66223,\n",
       "             323,    473]),\n",
       "  'labels': tensor([14924, 12770,   596,    11,   264,   389,   264,   426,   323, 21393,\n",
       "          56261])},\n",
       " {'input_ids': tensor([128000,     42,  14468,  31164,    574,   9405,    389,   6790,    220,\n",
       "              24,     11]),\n",
       "  'labels': tensor([14924, 46295,  1974,    11,   264,   389,  6790,   220,    24,    11,\n",
       "            220])},\n",
       " {'input_ids': tensor([128000,   1548,    574,   9405,    304,  52629,     11,   9635,     11]),\n",
       "  'labels': tensor([14924,  6260,  9405,   304,   220,    11,  9635,    11,   304])},\n",
       " {'input_ids': tensor([128000,  42315,  54755,    574,   9405,    389,   6841,    220,    605,\n",
       "              11,    220,   5926,     20]),\n",
       "  'labels': tensor([14924,   942,   323,   264,   389,  6841,   220,   605,    11,   220,\n",
       "           5926,    20,    11])},\n",
       " {'input_ids': tensor([128000,  29184,    356,    351,   3520,    574,   9405,    389,   5887,\n",
       "             220,   1114,     11,    220]),\n",
       "  'labels': tensor([14924,   432, 28884,  3520,    11,   264,   389,  5887,   220,  1114,\n",
       "             11,   220,  9378])},\n",
       " {'input_ids': tensor([128000,   2244,    942,  26056,    645,    574,   9405,    389,   3297,\n",
       "             220]),\n",
       "  'labels': tensor([14924,   278, 26056,   645,     6,   264,   389,  3297,   220,    21])},\n",
       " {'input_ids': tensor([128000,  29784,   1466,   2893,  23758,    574,   9405,    389,   5587,\n",
       "             220]),\n",
       "  'labels': tensor([14924,  1466, 86552, 23758,   323,  9405,   389,  5587,   220,   966])},\n",
       " {'input_ids': tensor([128000,   5176,    396,   6460,   6798,    374,    459,  50082,   3778,\n",
       "           12360,     11,   4632,   7690,     11,  17276,     11,    323,  39844,\n",
       "              11,   9405]),\n",
       "  'labels': tensor([14924,   352,  6460,  6798,   596,   264,  3778, 12360, 12360,    11,\n",
       "           7690,  7690,    11, 17276,    11,   323, 40550,    13,  3967,   389])},\n",
       " {'input_ids': tensor([128000,  40249,  44522,    574,   9405,    389,   6186,    220,     18,\n",
       "              11,    220,   6280]),\n",
       "  'labels': tensor([14924, 22144,   596,  9405,   389,  6186,   220,    18,    11,   220,\n",
       "           6280,    21])},\n",
       " {'input_ids': tensor([128000,  25763,  47477,    574,   9405,    389,   5887,    220,     18,\n",
       "              11,    220]),\n",
       "  'labels': tensor([14924,  2465,   596, 30652,   389,  5887,   220,    18,    11,   220,\n",
       "           5162])},\n",
       " {'input_ids': tensor([128000,   1548,    574,   9405,    389,   5887,    220,   1032,     11]),\n",
       "  'labels': tensor([14924,  6260,  9405,   304,   220,   220,    19,    11,   220])},\n",
       " {'input_ids': tensor([128000,  48781,  11746,     88,    574,   9405,    389,   5887,    220,\n",
       "            1627,     11,    220]),\n",
       "  'labels': tensor([14924, 23750,    88,   596, 11684,   389,  5887,   220,  1627,    11,\n",
       "            220,  6280])},\n",
       " {'input_ids': tensor([128000,   1548,    574,   9405,    389,   5936,    220,     22,     11,\n",
       "             220]),\n",
       "  'labels': tensor([14924,  6260,  9405,   304,   220,   220,    16,    11,   220,  5926])},\n",
       " {'input_ids': tensor([128000,  39727,  77339,    574,   9405,    389,   6790,    220,     19,\n",
       "              11,    220]),\n",
       "  'labels': tensor([14924,  1293,    11,  9405,   389,  6790,   220,    19,    11,   220,\n",
       "           6393])},\n",
       " {'input_ids': tensor([128000,  40052,  19197,   2596,    574,   9405,    389,   3297,    220,\n",
       "              21]),\n",
       "  'labels': tensor([14924,  6652,  2596,   596, 30652,   389,  3297,   220,    21,    11])},\n",
       " {'input_ids': tensor([128000,  40052,  19197,   2596,    574,   9405,    389]),\n",
       "  'labels': tensor([14924,  6652,  2596,   596, 30652,   389,  3297])},\n",
       " {'input_ids': tensor([128000,   1548,    706,   1403,   2911,     11,  44736,   7086,  78896,\n",
       "             323,  20643,     11]),\n",
       "  'labels': tensor([14924,  6260,  1027,  2911,   449,   264,    11,  7639,   323, 63264,\n",
       "             11,   449])},\n",
       " {'input_ids': tensor([128000,  40917,  73349,    574,   9405,    389,   6664,    220,     23,\n",
       "              11]),\n",
       "  'labels': tensor([14924,   676,   323,  9405,   389,  6664,   220,    23,    11,   220])},\n",
       " {'input_ids': tensor([128000,  10149,   9259,    574,   9405,    389,   6250,    220,    914,\n",
       "              11]),\n",
       "  'labels': tensor([14924,   279,   596,  9405,   389,  6250,   220,   914,    11,   220])},\n",
       " {'input_ids': tensor([128000,  42042,  84230,    574,   9405,    389,   6186,    220,    966,\n",
       "              11]),\n",
       "  'labels': tensor([14924,   488,   596,  9405,   389,  6186,   220,   966,    11,   220])},\n",
       " {'input_ids': tensor([128000,     41,  20103,  36444,  23503,    574,   9405,    304,   5960,\n",
       "           29604,     11,  31319]),\n",
       "  'labels': tensor([14924,  3431, 36444, 23503,   596,  9405,   389,  5960, 29604,    11,\n",
       "          31319, 34248])},\n",
       " {'input_ids': tensor([128000,   1548,    574,   9405,    389,   5887]),\n",
       "  'labels': tensor([14924,  6260,  9405,   304,   220,   220])},\n",
       " {'input_ids': tensor([128000,     44,  46125,  36772,    752,    574,   9405,    304,  35769,\n",
       "              11,   1561]),\n",
       "  'labels': tensor([14924,  6430, 36772,   752,    11,  9405,   389, 35769,    11,  1561,\n",
       "          16228])},\n",
       " {'input_ids': tensor([128000,     42,    589,  75314,  61245,  22464,    574,   9405,    389,\n",
       "            3297]),\n",
       "  'labels': tensor([14924, 46295,  1969,  4584, 22464,   596,   264,   389,  3297,   220])},\n",
       " {'input_ids': tensor([128000,  54049,   8233,  61245,  22464,    574,   9405,    389,   3297,\n",
       "             220,     19,     11,    220,   5926]),\n",
       "  'labels': tensor([14924,  8233, 61245, 22464,   596,   264,   389,  3297,   220,    19,\n",
       "             11,   220,  5926,    24])},\n",
       " {'input_ids': tensor([128000,  54049,   8233,    574,   9405,    311,  15466,  33412,  21353,\n",
       "           61245,  22464,  11151,    592,    263,     11,    264,   8013]),\n",
       "  'labels': tensor([14924,  8233, 61245,  9405,   304,   264,   323,   323,   323, 22464,\n",
       "            323,   592,   263,   323,   264,  8013, 69050])},\n",
       " {'input_ids': tensor([128000,     33,   6672,  17200,    574,   9405,    389,   5936,    220,\n",
       "              20]),\n",
       "  'labels': tensor([14924, 12770, 14013,   323,   264,   389,  5936,   220,    20,    11])},\n",
       " {'input_ids': tensor([128000,  76637,  16844,    574,   9405,    389,   7552,    220,   1544,\n",
       "              11,    220]),\n",
       "  'labels': tensor([14924, 26713,   596,   264,   389,  7552,   220,  1544,    11,   220,\n",
       "           7285])},\n",
       " {'input_ids': tensor([128000,    644,   4297,  31782,   1543,    574,   9405,    389,   6287,\n",
       "             220]),\n",
       "  'labels': tensor([14924,   279,  8096,  1543,    25,   264,   389,  6287,   220,  1682])},\n",
       " {'input_ids': tensor([128000,     44,   6751,     77,  50887,    574,   9405]),\n",
       "  'labels': tensor([14924,  6430,    77, 99233,   596,   264,   389])},\n",
       " {'input_ids': tensor([128000,     44,   6751,     77,  50887,    574,   9405]),\n",
       "  'labels': tensor([14924,  6430,    77, 99233,   596,   264,   389])},\n",
       " {'input_ids': tensor([128000,  29185,    689,  31248,    574,   9405,    389,   6664,    220,\n",
       "            1591]),\n",
       "  'labels': tensor([14924,   220,  9576,   323,  9405,   389,  6664,   220,  1591,    11])},\n",
       " {'input_ids': tensor([128000,  58916,   1286,  32666,   1543,    574,   9405,    389,   5651,\n",
       "             220,    508]),\n",
       "  'labels': tensor([14924, 19736, 32666,  1543,   596,  9405,   389,  5651,   220,   508,\n",
       "             11])},\n",
       " {'input_ids': tensor([128000,     34,    349,  92529,   7211,    574,   9405,    389,   3297,\n",
       "             220]),\n",
       "  'labels': tensor([14924,  7709, 92529,  7211,   323,  9405,   389,  3297,   220,   975])},\n",
       " {'input_ids': tensor([128000,     41,  86449,  39274,    574,   9405,    389,   6841,    220,\n",
       "             777,     11]),\n",
       "  'labels': tensor([14924,  3431, 27687,   596,  9405, 80002,  6841,   220,   777,    11,\n",
       "            220])},\n",
       " {'input_ids': tensor([128000,  86383,  13951,  11283,    574,   9405,    389,   6664,    220,\n",
       "              19,     11,    220,   6393]),\n",
       "  'labels': tensor([14924,   480, 11283,   596,  9405,   389,  6664,   220,    19,    11,\n",
       "            220,  6393,    21])},\n",
       " {'input_ids': tensor([128000,  80090,  88822,    574,   9405,    389,   5936,    220,    508,\n",
       "              11]),\n",
       "  'labels': tensor([14924, 35766,    11,  9405,   389,  5936,   220,   508,    11,   220])},\n",
       " {'input_ids': tensor([128000,  32641,   2734,  13330,    574,   9405,    389,   5587,    220,\n",
       "             777]),\n",
       "  'labels': tensor([14924, 13462, 29818,    11,  9405,   389,  5587,   220,   777,    11])},\n",
       " {'input_ids': tensor([128000,     39,   8564,  14603,   1466,    574,   9405,    389,   5887,\n",
       "             220,   1627]),\n",
       "  'labels': tensor([14924,   556, 14603,  1466,    11,  9405,   389,  5887,   220,  1627,\n",
       "             11])},\n",
       " {'input_ids': tensor([128000,     41,  21178,   9973,    331,    574,   9405,    389,   6790,\n",
       "             220,     24]),\n",
       "  'labels': tensor([14924,  3431,  9973,   331,   323,  9405,   389,  6790,   220,    24,\n",
       "             11])},\n",
       " {'input_ids': tensor([128000,     44,  16094,    648,   9259,    574,   9405,    389,   5651,\n",
       "             220,   1591]),\n",
       "  'labels': tensor([14924,  6430,   648,   596,    11,  9405,   389,  6790,   220,  1591,\n",
       "             11])},\n",
       " {'input_ids': tensor([128000,  46324,  26577,   3816,  77731,    574,   9405,    389,   6186,\n",
       "             220,    966]),\n",
       "  'labels': tensor([14924, 26577, 94005, 77731,    11,  9405,   389,  6186,   220,   966,\n",
       "             11])},\n",
       " {'input_ids': tensor([128000,  86332,    689,  52290,    574,   9405,    389,   6250,    220,\n",
       "             508]),\n",
       "  'labels': tensor([14924,   648,   596,    11,  9405, 79663,  6250,   220,   508,    11])},\n",
       " {'input_ids': tensor([128000,  87643,  19178,    574,   9405,    389,   6841,    220,    717,\n",
       "              11]),\n",
       "  'labels': tensor([14924,   323,   596,   459,   389,  6841,   220,   717,    11,   220])},\n",
       " {'input_ids': tensor([128000,  63602,    435,  18693,    574,   9405,    389,   6790,    220,\n",
       "            1691,     11]),\n",
       "  'labels': tensor([14924, 13222, 18693,   596,  9405,   389,  6790,   220,  1691,    11,\n",
       "            220])},\n",
       " {'input_ids': tensor([128000,   2059,    404,   3258,   7553,     43,   8511,    574,   9405,\n",
       "             389]),\n",
       "  'labels': tensor([14924, 44994,  3258,    11,    43,  8511,   596,  9405,   389,  5936])},\n",
       " {'input_ids': tensor([128000,     37,  61055,  29838,  14075,    374,    459,  27373,   3778,\n",
       "           24577,   3967,    369,   1077,   8147,  24601,     11,   9405,    304,\n",
       "           15004]),\n",
       "  'labels': tensor([14924,  8768, 29838, 14075,    11,   459,  3778,  3778, 24577,  3967,\n",
       "            369,  1077, 21933, 24601,   304, 21933,   389, 15004,   884])},\n",
       " {'input_ids': tensor([128000,  48346,  34090,  67372,    574,   9405,    389,   6664,    220,\n",
       "              23,     11,    220,   6393]),\n",
       "  'labels': tensor([14924,   328, 67372,    11,  9405,   389,  6287,   220,    23,    11,\n",
       "            220,  6393,    24])},\n",
       " {'input_ids': tensor([128000,     50,    750,   8771,    574,   9405,    389,   6841,    220,\n",
       "              21]),\n",
       "  'labels': tensor([14924, 42341,   596,   596,  9405,   389,  6841,   220,    21,    11])},\n",
       " {'input_ids': tensor([128000,     50,  61315,  11746,     74,    574,   9405,    389,   6790,\n",
       "             220]),\n",
       "  'labels': tensor([14924, 42341, 11746,    74,   596,  9405, 10455,  6790,   220,   914])},\n",
       " {'input_ids': tensor([128000,     35,  36135,  27109,    574,   9405,    304,   1561,   4356,\n",
       "            4409,    389]),\n",
       "  'labels': tensor([14924,  6430, 43227,   596,  9405,   389,  1561,  4356,  4409,   389,\n",
       "           6186])},\n",
       " {'input_ids': tensor([128000,  83862,    393,   1897,  14657,    574,   9405,    389,   5936,\n",
       "             220,   1682]),\n",
       "  'labels': tensor([14924,  7250,  1897, 14657,   596,  9405,   389,  5936,   220,  1682,\n",
       "             11])},\n",
       " {'input_ids': tensor([128000,  29185,  64452,  20832,    574,   9405,    389,   6790,    220,\n",
       "              18,     11,    220]),\n",
       "  'labels': tensor([14924,   220,   473,    11,  9405,   389,  6790,   220,    18,    11,\n",
       "            220,  5162])},\n",
       " {'input_ids': tensor([128000,  90174,  26224,    574,   9405,    389,   5936,    220,    868,\n",
       "              11]),\n",
       "  'labels': tensor([14924,   374,   374,  9405,   389,  5936,   220,   868,    11,   220])},\n",
       " {'input_ids': tensor([128000,  80469,  49544,   1169,    574,   9405,    389,   6664,    220,\n",
       "              20,     11]),\n",
       "  'labels': tensor([14924, 92714,  1169,   596,  9405,   389,  6664,   220,    20,    11,\n",
       "            220])},\n",
       " {'input_ids': tensor([128000,     45,   4306,    648,   5896,   1543,    574,   9405,    389,\n",
       "            5651]),\n",
       "  'labels': tensor([14924,   276,   648,  5896,  1543,   596,  9405,   389,  5651,   220])},\n",
       " {'input_ids': tensor([128000,   4873,     75,    553,    666,  20110,    574,   9405,    389,\n",
       "            6287,    220]),\n",
       "  'labels': tensor([14924,   488, 61670,   666, 20110,   374,  9405,   389,  6287,   220,\n",
       "             22])},\n",
       " {'input_ids': tensor([128000,     50,  24155,  13805,   1039,    574,   9405,    389,   5887,\n",
       "             220]),\n",
       "  'labels': tensor([14924, 42341, 13805,  1039,   596,  9405,   389,  5887,   220,  1627])},\n",
       " {'input_ids': tensor([128000,    697,   2423,   3161,    388,  33076,    574,   9405,    389,\n",
       "            5587,    220]),\n",
       "  'labels': tensor([14924,    25,  3161,   388, 33076,   596, 30652,   389,  5587,   220,\n",
       "           1313])},\n",
       " {'input_ids': tensor([128000,  69404,   2259,    622,  62117,    574,   9405]),\n",
       "  'labels': tensor([14924,  2259,   622, 62117,   323,  9405,   389])},\n",
       " {'input_ids': tensor([128000,  69404,   2259,    622,  62117,    574,   9405,    311,  20142,\n",
       "           12565]),\n",
       "  'labels': tensor([14924,  2259,   622, 62117,   323,  9405,   389,   264, 12565, 29179])},\n",
       " {'input_ids': tensor([128000,  79039,  91668,  14075,    574,   9405,    389,   6841,    220,\n",
       "             717,     11,    220]),\n",
       "  'labels': tensor([14924,  9454, 14075,   596,  9405,   389,  6841,   220,   717,    11,\n",
       "            220,  3753])},\n",
       " {'input_ids': tensor([128000,  76109,  27329,    574,   9405,    389,   6287,    220,    508,\n",
       "              11,    220]),\n",
       "  'labels': tensor([14924, 52196,   374,  9405,   389,  6287,   220,   508,    11,   220,\n",
       "           4468])},\n",
       " {'input_ids': tensor([128000,  72526,  28574,    574,   9405,    389,   6287,    220,    868,\n",
       "              11]),\n",
       "  'labels': tensor([14924, 45315,   323, 30652,   389,  6287,   220,   868,    11,   220])},\n",
       " {'input_ids': tensor([128000,  96056,  28163,  27268,  81265,    574,   9405,    389,   6841,\n",
       "             220,   1313,     11,    220]),\n",
       "  'labels': tensor([14924,   346, 27268, 81265,   596,  9405,   389,  6841,   220,  1313,\n",
       "             11,   220,  3753])},\n",
       " {'input_ids': tensor([128000,  85148,   2563,   3935,    574,   9405,    389,   7552,    220,\n",
       "            1419,     11]),\n",
       "  'labels': tensor([14924,   432,  3935,   323,  9405,   389,  7552,   220,  1419,    11,\n",
       "            220])},\n",
       " {'input_ids': tensor([128000,  84978,   1226,  70828,    574,   9405,    389,   5587,    220,\n",
       "              22,     11]),\n",
       "  'labels': tensor([14924,   596, 70828,   323,  9405,   389,  5587,   220,    22,    11,\n",
       "            220])},\n",
       " {'input_ids': tensor([128000,  16589,  22157,  59336,    574,   9405,    389,   6250,    220,\n",
       "            1591,     11]),\n",
       "  'labels': tensor([14924, 22157, 88085,   323,  9405,   389,  6250,   220,  1591,    11,\n",
       "            220])},\n",
       " {'input_ids': tensor([128000,  29305,    978,  36427,  21510,    574,   9405,    304,   1708,\n",
       "           86656,  42607]),\n",
       "  'labels': tensor([14924, 24387, 36427, 21510,    11,  9405,   389, 25048, 86656, 42607,\n",
       "             11])},\n",
       " {'input_ids': tensor([128000,  12331,    290,  68825,  67359,    574,   9405,    389,   6250,\n",
       "             220]),\n",
       "  'labels': tensor([14924,  8950,  6406, 67359,   596,  9405,   389,  6250,   220,   966])},\n",
       " {'input_ids': tensor([128000,  35725,   8047,   1901,    616,     86,   1916,    574,   9405,\n",
       "             389]),\n",
       "  'labels': tensor([14924,   365,  1901,   616,    86,  1916,   374,  9405,   389,  5936])},\n",
       " {'input_ids': tensor([128000,     39,   5164,  44685,    574,   9405,    389,   6287,    220,\n",
       "             975]),\n",
       "  'labels': tensor([14924,   556, 44685,   596,  9405,   389,  6287,   220,   975,    11])},\n",
       " {'input_ids': tensor([128000,     34,  91712,  58131,    574,   9405,    389,   6287,    220,\n",
       "             966,     11]),\n",
       "  'labels': tensor([14924,  7709,   596,   323,  9405,   389,  6287,   220,   966,    11,\n",
       "            220])}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
